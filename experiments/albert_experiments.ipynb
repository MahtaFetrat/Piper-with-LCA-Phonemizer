{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XjAPkfq7SF87",
        "JGEUIrbi9kNH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/roshan-research/hazm.git\n",
        "\n",
        "# Replace Python version constraint\n",
        "!sed -i 's/python= \">=3.8, <3.12\"/python= \">=3.8\"/g' hazm/pyproject.toml\n",
        "\n",
        "# Replace numpy version constraint\n",
        "!sed -i 's/numpy = \"~1.24\"/numpy = \"1.26\"/g' hazm/pyproject.toml\n",
        "\n",
        "!pip install ./hazm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VmpCjdrS3scI",
        "outputId": "e1127b71-6bb2-4d0f-97ab-029288e9fe75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hazm'...\n",
            "remote: Enumerating objects: 8674, done.\u001b[K\n",
            "remote: Counting objects: 100% (2005/2005), done.\u001b[K\n",
            "remote: Compressing objects: 100% (394/394), done.\u001b[K\n",
            "remote: Total 8674 (delta 1830), reused 1611 (delta 1611), pack-reused 6669 (from 1)\u001b[K\n",
            "Receiving objects: 100% (8674/8674), 23.95 MiB | 18.99 MiB/s, done.\n",
            "Resolving deltas: 100% (5593/5593), done.\n",
            "Processing ./hazm\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fasttext-wheel<0.10.0,>=0.9.2 (from hazm==0.10.0)\n",
            "  Downloading fasttext_wheel-0.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting flashtext<3.0,>=2.7 (from hazm==0.10.0)\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gensim<5.0.0,>=4.3.1 (from hazm==0.10.0)\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.12/dist-packages (from hazm==0.10.0) (3.9.1)\n",
            "Collecting numpy==1.26 (from hazm==0.10.0)\n",
            "  Downloading numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-crfsuite<0.10.0,>=0.9.9 (from hazm==0.10.0)\n",
            "  Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from hazm==0.10.0) (1.6.1)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel<0.10.0,>=0.9.2->hazm==0.10.0)\n",
            "  Downloading pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm==0.10.0) (75.2.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim<5.0.0,>=4.3.1->hazm==0.10.0) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim<5.0.0,>=4.3.1->hazm==0.10.0) (7.4.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk<4.0.0,>=3.8.1->hazm==0.10.0) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk<4.0.0,>=3.8.1->hazm==0.10.0) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk<4.0.0,>=3.8.1->hazm==0.10.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk<4.0.0,>=3.8.1->hazm==0.10.0) (4.67.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm==0.10.0) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm==0.10.0) (2.0.0)\n",
            "Downloading numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasttext_wheel-0.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: hazm, flashtext\n",
            "  Building wheel for hazm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hazm: filename=hazm-0.10.0-py3-none-any.whl size=894002 sha256=07f169c9e78437dcc93933649b6aa1fa4153175726b476e7226ed5471fce5527\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2zfr14oz/wheels/bc/2f/5d/575b4342ef263c10935d0176afc20405e8ffbdef19d48a9cf8\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9300 sha256=a2d0a1cfb219254d33e2a617e21eea1355ee804a0bb193f97def13dea2b08914\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/24/da/4d994d7a27cfc73a4e513a669fbeec4a71f871fe245a81977f\n",
            "Successfully built hazm flashtext\n",
            "Installing collected packages: flashtext, python-crfsuite, pybind11, numpy, fasttext-wheel, gensim, hazm\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.0 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.0 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fasttext-wheel-0.9.2 flashtext-2.7 gensim-4.4.0 hazm-0.10.0 numpy-1.26.0 pybind11-3.0.1 python-crfsuite-0.9.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "6c23be60a0a84ad794787ed6aab3e065"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers onnx optimum --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABFHry_o7Uwt",
        "outputId": "ba63201a-3754-42c6-dd0d-4374462ce416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting optimum\n",
            "  Downloading optimum-2.0.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.3)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.12/dist-packages (from optimum) (2.8.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11->optimum) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11->optimum) (3.0.3)\n",
            "Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optimum-2.0.0-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.3/162.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, optimum\n",
            "Successfully installed onnx-1.19.1 optimum-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvmo8U2e75zt",
        "outputId": "bb0ed622-1efc-4245-b0eb-4b3058899687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.9.23)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.23.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade optimum[onnxruntime]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DJEfh6m8c1y",
        "outputId": "3f8645ad-008d-49c8-dd9e-46d92adccf63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optimum[onnxruntime] in /usr/local/lib/python3.12/dist-packages (2.0.0)\n",
            "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.12/dist-packages (from optimum[onnxruntime]) (4.57.1)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.12/dist-packages (from optimum[onnxruntime]) (2.8.0+cu126)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from optimum[onnxruntime]) (25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optimum[onnxruntime]) (2.0.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from optimum[onnxruntime]) (0.36.0)\n",
            "Collecting optimum-onnx[onnxruntime] (from optimum[onnxruntime])\n",
            "  Downloading optimum_onnx-0.0.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (0.6.2)\n",
            "Collecting transformers>=4.29 (from optimum[onnxruntime])\n",
            "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (from optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (1.19.1)\n",
            "Requirement already satisfied: onnxruntime>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (1.23.2)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.29->optimum[onnxruntime])\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (5.29.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11->optimum[onnxruntime]) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11->optimum[onnxruntime]) (3.0.3)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (0.5.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (2025.10.5)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (10.0)\n",
            "Downloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optimum_onnx-0.0.3-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m192.3/192.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers, optimum-onnx\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.1\n",
            "    Uninstalling transformers-4.57.1:\n",
            "      Successfully uninstalled transformers-4.57.1\n",
            "Successfully installed optimum-onnx-0.0.3 tokenizers-0.21.4 transformers-4.55.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1vdInn73cHsMCszktCqOT1zOtz0ukBwUp # old version 1A7ENXr8BkZ0j38nJqb2Lm883BnP_N1pH\n",
        "\n",
        "!unzip ezafe_model_quantized.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4ATkdka6qWl",
        "outputId": "2311bb82-4dd6-4ad5-bf17-5ea63d5c6433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1vdInn73cHsMCszktCqOT1zOtz0ukBwUp\n",
            "From (redirected): https://drive.google.com/uc?id=1vdInn73cHsMCszktCqOT1zOtz0ukBwUp&confirm=t&uuid=89b357ba-917f-49c4-898b-299073f6ae9e\n",
            "To: /content/ezafe_model_quantized.zip\n",
            "100% 36.2M/36.2M [00:01<00:00, 30.5MB/s]\n",
            "Archive:  ezafe_model_quantized.zip\n",
            "   creating: ezafe_model_quantized/\n",
            "  inflating: ezafe_model_quantized/model_quantized.onnx  \n",
            "  inflating: ezafe_model_quantized/ort_config.json  \n",
            "  inflating: ezafe_model_quantized/config.json  \n",
            "  inflating: ezafe_model_quantized/tokenizer_config.json  \n",
            "  inflating: ezafe_model_quantized/special_tokens_map.json  \n",
            "  inflating: ezafe_model_quantized/spiece.model  \n",
            "  inflating: ezafe_model_quantized/tokenizer.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from optimum.onnxruntime import ORTModelForTokenClassification\n",
        "from transformers import AutoTokenizer\n",
        "import unicodedata\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Load quantized ezafe model\n",
        "quantized_model_path = \"ezafe_model_quantized\"\n",
        "model = ORTModelForTokenClassification.from_pretrained(quantized_model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(quantized_model_path)\n",
        "\n",
        "def predict_ezafe_simple(text, model, tokenizer):\n",
        "    words = text.split()\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        words,\n",
        "        is_split_into_words=True,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_labels = torch.argmax(predictions, dim=-1)\n",
        "\n",
        "    word_ids = inputs.word_ids()\n",
        "    results = []\n",
        "\n",
        "    previous_word_idx = None\n",
        "    for i, word_idx in enumerate(word_ids):\n",
        "        if word_idx is not None and word_idx != previous_word_idx:\n",
        "            if word_idx < len(words):\n",
        "                results.append({\n",
        "                    'word': words[word_idx],\n",
        "                    'needs_ezafe': bool(predicted_labels[0][i].item()),\n",
        "                    'confidence': float(predictions[0][i][predicted_labels[0][i]].item())\n",
        "                })\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXM1IEzW6uME",
        "outputId": "0e22f34f-a6f5-492d-c7a2-05afb1486eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Multiple distributions found for package optimum. Picked distribution: optimum-onnx\n",
            "Could not find any ONNX files with standard file name model.onnx, files found: [PosixPath('model_quantized.onnx')]. Please make sure to pass a `file_name` and/or `subfolder` argument to `from_pretrained` when loading an ONNX file with non-standard file names.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psutil\n",
        "import torch\n",
        "import onnx\n",
        "from optimum.onnxruntime import ORTModelForTokenClassification\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "\n",
        "def analyze_onnx_and_base_model(model_dir, base_model_name):\n",
        "    \"\"\"\n",
        "    Reports detailed size, memory, and parameter info for an ONNX model\n",
        "    derived from a known Hugging Face base model (e.g., ALBERT, BERT, etc.).\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"ğŸ“¦ Loading ONNX model from: {model_dir}\")\n",
        "    process = psutil.Process(os.getpid())\n",
        "    before_mem = process.memory_info().rss / (1024 * 1024)\n",
        "\n",
        "    # Load ONNX model and tokenizer\n",
        "    model = ORTModelForTokenClassification.from_pretrained(model_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "    after_mem = process.memory_info().rss / (1024 * 1024)\n",
        "    load_mem = after_mem - before_mem\n",
        "    print(f\"âœ… Model loaded. Memory used during load: {load_mem:.2f} MB\\n\")\n",
        "\n",
        "    # --- Disk size ---\n",
        "    total_disk_bytes = sum(\n",
        "        os.path.getsize(os.path.join(dp, f))\n",
        "        for dp, _, filenames in os.walk(model_dir)\n",
        "        for f in filenames\n",
        "    )\n",
        "    total_disk_mb = total_disk_bytes / (1024 * 1024)\n",
        "    print(f\"ğŸ“‚ Model size on disk: {total_disk_mb:.2f} MB\")\n",
        "\n",
        "    # --- Check quantization ---\n",
        "    onnx_files = [f for f in os.listdir(model_dir) if f.endswith(\".onnx\")]\n",
        "    quantized = False\n",
        "    if onnx_files:\n",
        "        onnx_path = os.path.join(model_dir, onnx_files[0])\n",
        "        print(f\"ğŸ” Found ONNX file: {onnx_files[0]}\")\n",
        "        model_proto = onnx.load(onnx_path)\n",
        "        quantized = any(\"Quantize\" in n.op_type or \"Dequantize\" in n.op_type for n in model_proto.graph.node)\n",
        "        print(f\"âš™ï¸  Quantized model: {'âœ… Yes' if quantized else 'âŒ No'}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ No .onnx file found in model directory.\")\n",
        "\n",
        "    # --- Tokenizer info ---\n",
        "    vocab_size = len(tokenizer.get_vocab())\n",
        "    print(f\"ğŸ”¤ Tokenizer vocab size: {vocab_size:,}\\n\")\n",
        "\n",
        "    # --- Load original HF model to get exact parameter count ---\n",
        "    print(f\"ğŸ§  Loading base model: {base_model_name}\")\n",
        "    base_model = AutoModelForTokenClassification.from_pretrained(base_model_name, num_labels=model.config.num_labels)\n",
        "\n",
        "    total_params = sum(p.numel() for p in base_model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
        "    param_bytes = sum(p.numel() * p.element_size() for p in base_model.parameters())\n",
        "\n",
        "    print(f\"âœ… Base model loaded.\")\n",
        "    print(f\"ğŸ”¹ Total parameters: {total_params:,}\")\n",
        "    print(f\"ğŸ”¹ Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"ğŸ’¾ Weight memory (float32): {param_bytes / (1024 * 1024):.2f} MB\\n\")\n",
        "\n",
        "    # --- Summary ---\n",
        "    print(\"ğŸ“Š SUMMARY\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"ğŸ§© Base model                : {base_model_name}\")\n",
        "    print(f\"ğŸ“‚ ONNX model size on disk    : {total_disk_mb:.2f} MB\")\n",
        "    print(f\"ğŸ§  Total parameters (exact)   : {total_params:,}\")\n",
        "    print(f\"ğŸ’¾ Weight memory (float32)    : {param_bytes / (1024 * 1024):.2f} MB\")\n",
        "    print(f\"âš™ï¸  Quantized ONNX model       : {'Yes' if quantized else 'No'}\")\n",
        "    print(f\"ğŸ”¤ Tokenizer vocab size       : {vocab_size:,}\")\n",
        "    print(f\"ğŸ“ˆ RAM used after load (ONNX) : {load_mem:.2f} MB\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    return {\n",
        "        \"base_model\": base_model_name,\n",
        "        \"disk_size_MB\": total_disk_mb,\n",
        "        \"parameters\": total_params,\n",
        "        \"weight_memory_MB\": param_bytes / (1024 * 1024),\n",
        "        \"quantized\": quantized,\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"load_memory_MB\": load_mem\n",
        "    }\n",
        "\n",
        "\n",
        "# ---- Run the analyzer ----\n",
        "if __name__ == \"__main__\":\n",
        "    model_dir = \"ezafe_model_quantized\"\n",
        "    base_model_name = \"HooshvareLab/albert-fa-zwnj-base-v2\"\n",
        "    report = analyze_onnx_and_base_model(model_dir, base_model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARL_hZe6pm0D",
        "outputId": "056a73bf-78b3-4891-c12a-4b16ec2943cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not find any ONNX files with standard file name model.onnx, files found: [PosixPath('model_quantized.onnx')]. Please make sure to pass a `file_name` and/or `subfolder` argument to `from_pretrained` when loading an ONNX file with non-standard file names.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Loading ONNX model from: ezafe_model_quantized\n",
            "âœ… Model loaded. Memory used during load: 68.50 MB\n",
            "\n",
            "ğŸ“‚ Model size on disk: 41.38 MB\n",
            "ğŸ” Found ONNX file: model_quantized.onnx\n",
            "âš™ï¸  Quantized model: âœ… Yes\n",
            "ğŸ”¤ Tokenizer vocab size: 30,000\n",
            "\n",
            "ğŸ§  Loading base model: HooshvareLab/albert-fa-zwnj-base-v2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at HooshvareLab/albert-fa-zwnj-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Base model loaded.\n",
            "ğŸ”¹ Total parameters: 11,094,530\n",
            "ğŸ”¹ Trainable parameters: 11,094,530\n",
            "ğŸ’¾ Weight memory (float32): 42.32 MB\n",
            "\n",
            "ğŸ“Š SUMMARY\n",
            "------------------------------------------------------------\n",
            "ğŸ§© Base model                : HooshvareLab/albert-fa-zwnj-base-v2\n",
            "ğŸ“‚ ONNX model size on disk    : 41.38 MB\n",
            "ğŸ§  Total parameters (exact)   : 11,094,530\n",
            "ğŸ’¾ Weight memory (float32)    : 42.32 MB\n",
            "âš™ï¸  Quantized ONNX model       : Yes\n",
            "ğŸ”¤ Tokenizer vocab size       : 30,000\n",
            "ğŸ“ˆ RAM used after load (ONNX) : 68.50 MB\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Test"
      ],
      "metadata": {
        "id": "-X-bLa6G1Quw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import Normalizer\n",
        "\n",
        "normalizer = Normalizer()"
      ],
      "metadata": {
        "id": "MD_NQH-q3m54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install regex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SV29qmxH5BDx",
        "outputId": "e9598d4c-f009-48a0-a765-69bedbcff769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "import string\n",
        "\n",
        "def remove_punctuation_persian(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove all punctuation and symbols (English + Persian + Unicode).\n",
        "    Keeps only letters, digits, and spaces.\n",
        "    \"\"\"\n",
        "    punct_english = string.punctuation\n",
        "    punct_persian = \"ØŒØ›ØŸÙªÙ«Ù¬Ù­Ù€Â«Â»â€¹â€ºâ€¦â€“â€”âˆ’Ù€\"\n",
        "\n",
        "    # Matches all punctuation & symbols across Unicode + specific Persian marks\n",
        "    pattern = r\"[\\p{P}\\p{S}\" + re.escape(punct_english + punct_persian) + r\"]+\"\n",
        "\n",
        "    cleaned = re.sub(pattern, \" \", text)\n",
        "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
        "    return cleaned\n"
      ],
      "metadata": {
        "id": "7tD8Gu-f4kWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "def compute_ezafe_confusion_predict(csv_path, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Evaluate Ezafe prediction model on dataset CSV.\n",
        "    Returns TP, TN, FP, FN counts and prints performance metrics.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    tp = tn = fp = fn = 0\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        sentence = row[\"grapheme\"]\n",
        "        gold_tags = ast.literal_eval(row[\"ezafe_tags\"])\n",
        "\n",
        "        # Gold: dict[word] = True/False\n",
        "        gold_dict = {\n",
        "            clean_word: ez\n",
        "            for word, ez in gold_tags\n",
        "            if (clean_word := remove_punctuation_persian(word).strip())\n",
        "        }\n",
        "\n",
        "        # Predicted: dict[word] = True/False\n",
        "        predicted = predict_ezafe_simple(normalizer.normalize(sentence), model, tokenizer)\n",
        "        pred_dict = {remove_punctuation_persian(entry[\"word\"]): entry[\"needs_ezafe\"] for entry in predicted}\n",
        "\n",
        "        # Compare gold vs predicted (based on gold order)\n",
        "        for word, gold_ez in gold_dict.items():\n",
        "            pred_ez = pred_dict.get(word, False)\n",
        "            if gold_ez and pred_ez:\n",
        "                tp += 1\n",
        "            elif not gold_ez and not pred_ez:\n",
        "                tn += 1\n",
        "            elif not gold_ez and pred_ez:\n",
        "                fp += 1\n",
        "            elif gold_ez and not pred_ez:\n",
        "                fn += 1\n",
        "\n",
        "    # Metrics\n",
        "    precision = tp / (tp + fp) if (tp + fp) else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) else 0\n",
        "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0\n",
        "    total = tp + tn + fp + fn\n",
        "\n",
        "    print(f\"True Positives (TP): {tp}\")\n",
        "    print(f\"True Negatives (TN): {tn}\")\n",
        "    print(f\"False Positives (FP): {fp}\")\n",
        "    print(f\"False Negatives (FN): {fn}\")\n",
        "    print(f\"Total tokens: {total}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    return {\"TP\": tp, \"TN\": tn, \"FP\": fp, \"FN\": fn,\n",
        "            \"Precision\": precision, \"Recall\": recall, \"F1\": f1}\n",
        "\n",
        "\n",
        "compute_ezafe_confusion_predict('augSentenceBench.csv', model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLuBYlGZ2QuU",
        "outputId": "c322f974-0b96-43f8-c0d0-828ddf977d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Positives (TP): 519\n",
            "True Negatives (TN): 2451\n",
            "False Positives (FP): 33\n",
            "False Negatives (FN): 31\n",
            "Total tokens: 3034\n",
            "Precision: 0.9402\n",
            "Recall: 0.9436\n",
            "F1 Score: 0.9419\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'TP': 519,\n",
              " 'TN': 2451,\n",
              " 'FP': 33,\n",
              " 'FN': 31,\n",
              " 'Precision': 0.9402173913043478,\n",
              " 'Recall': 0.9436363636363636,\n",
              " 'F1': 0.9419237749546279}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speed Test"
      ],
      "metadata": {
        "id": "VsC45jSW1LwS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjAPkfq7SF87"
      },
      "source": [
        "# Get Evaluation Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ddGfGO2s4KX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwCG0jX-88nQ",
        "outputId": "8b9926f7-9092-42bf-c76e-9e25e512b701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-07 15:00:57--  https://huggingface.co/datasets/MahtaFetrat/SentenceBench/raw/main/SentenceBench.csv\n",
            "Resolving huggingface.co (huggingface.co)... 3.169.137.5, 3.169.137.119, 3.169.137.19, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.169.137.5|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 56026 (55K) [text/plain]\n",
            "Saving to: â€˜SentenceBench.csvâ€™\n",
            "\n",
            "SentenceBench.csv   100%[===================>]  54.71K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2025-11-07 15:00:57 (13.6 MB/s) - â€˜SentenceBench.csvâ€™ saved [56026/56026]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/datasets/MahtaFetrat/SentenceBench/raw/main/SentenceBench.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJO-UAPDQvcb"
      },
      "outputs": [],
      "source": [
        "sentence_bench = pd.read_csv('SentenceBench.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlYbrnUa9LAN",
        "outputId": "ad5ad5cd-055f-430d-f591-41103f6776c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     dataset                              grapheme  \\\n",
              "0  homograph                  Ù…Ù† Ù‚Ø¯Ø± ØªÙˆ Ø±Ø§ Ù…ÛŒâ€ŒØ¯Ø§Ù†Ù…   \n",
              "1  homograph  Ø§Ø² Ù‚Ø¶Ø§ÛŒ Ø§Ù„Ù‡ÛŒ Ø¨Ù‡ Ù‚Ø¯Ø± Ø§Ù„Ù‡ÛŒ Ù¾Ù†Ø§Ù‡ Ù…ÛŒâ€ŒØ¨Ø±Ù…   \n",
              "2  homograph                Ø¨Ù‡ Ø¯Ø³Øª Ùˆ ØµÙˆØ±ØªÙ… Ú©Ø±Ù… Ø²Ø¯Ù…   \n",
              "\n",
              "                                             phoneme homograph word  \\\n",
              "0                          man qadr-e to rA mi-dAnam            Ù‚Ø¯Ø±   \n",
              "1  ?az qazAy ?elAhi be qadar-e ?elAhi panAh mi-baram            Ù‚Ø¯Ø±   \n",
              "2                      be dast-o suratam kerem zadam            Ú©Ø±Ù…   \n",
              "\n",
              "  pronunciation  \n",
              "0          qadr  \n",
              "1         qadar  \n",
              "2         kerem  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-611f5def-b0b3-4a0a-bd72-42169242e53b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset</th>\n",
              "      <th>grapheme</th>\n",
              "      <th>phoneme</th>\n",
              "      <th>homograph word</th>\n",
              "      <th>pronunciation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>homograph</td>\n",
              "      <td>Ù…Ù† Ù‚Ø¯Ø± ØªÙˆ Ø±Ø§ Ù…ÛŒâ€ŒØ¯Ø§Ù†Ù…</td>\n",
              "      <td>man qadr-e to rA mi-dAnam</td>\n",
              "      <td>Ù‚Ø¯Ø±</td>\n",
              "      <td>qadr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>homograph</td>\n",
              "      <td>Ø§Ø² Ù‚Ø¶Ø§ÛŒ Ø§Ù„Ù‡ÛŒ Ø¨Ù‡ Ù‚Ø¯Ø± Ø§Ù„Ù‡ÛŒ Ù¾Ù†Ø§Ù‡ Ù…ÛŒâ€ŒØ¨Ø±Ù…</td>\n",
              "      <td>?az qazAy ?elAhi be qadar-e ?elAhi panAh mi-baram</td>\n",
              "      <td>Ù‚Ø¯Ø±</td>\n",
              "      <td>qadar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>homograph</td>\n",
              "      <td>Ø¨Ù‡ Ø¯Ø³Øª Ùˆ ØµÙˆØ±ØªÙ… Ú©Ø±Ù… Ø²Ø¯Ù…</td>\n",
              "      <td>be dast-o suratam kerem zadam</td>\n",
              "      <td>Ú©Ø±Ù…</td>\n",
              "      <td>kerem</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-611f5def-b0b3-4a0a-bd72-42169242e53b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-611f5def-b0b3-4a0a-bd72-42169242e53b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-611f5def-b0b3-4a0a-bd72-42169242e53b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-90517d68-5c59-4be9-8432-35eb8cc4c3ac\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-90517d68-5c59-4be9-8432-35eb8cc4c3ac')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-90517d68-5c59-4be9-8432-35eb8cc4c3ac button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "sentence_bench",
              "summary": "{\n  \"name\": \"sentence_bench\",\n  \"rows\": 400,\n  \"fields\": [\n    {\n      \"column\": \"dataset\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"homograph\",\n          \"mana-tts\",\n          \"commonvoice\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"grapheme\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 400,\n        \"samples\": [\n          \"\\u0622\\u06cc\\u0627 \\u0628\\u0627\\u06cc\\u062f \\u062d\\u0642\\u06cc\\u0642\\u062a \\u0631\\u0627 \\u0628\\u0647 \\u0622\\u0646\\u200c\\u0647\\u0627 \\u0628\\u06af\\u0648\\u06cc\\u06cc\\u0645\\u061f\",\n          \"\\u06a9\\u0647 \\u067e\\u06cc\\u0634 \\u0627\\u0632 \\u0627\\u0646\\u0642\\u0644\\u0627\\u0628 \\u0628\\u0647 \\u062e\\u0648\\u0627\\u0628\\u06af\\u0627\\u0647 \\u062f\\u062e\\u062a\\u0631\\u0627\\u0646 \\u0648 \\u0632\\u0646\\u0627\\u0646 \\u0646\\u0627\\u0628\\u06cc\\u0646\\u0627 \\u0627\\u062e\\u062a\\u0635\\u0627\\u0635\\u200c\\u06cc\\u0627\\u0641\\u062a\\u0647 \\u0628\\u0648\\u062f. \\u0627\\u063a\\u0644\\u0628 \\u0632\\u0646\\u0627\\u0646\\u06cc \\u06a9\\u0647 \\u062f\\u0631 \\u0627\\u06cc\\u0646 \\u062e\\u0648\\u0627\\u0628\\u06af\\u0627\\u0647 \\u0632\\u0646\\u062f\\u06af\\u06cc \\u0645\\u06cc\\u200c\\u06a9\\u0631\\u062f\\u0646\\u062f\\u060c \",\n          \"\\u062f\\u0648\\u062f \\u0648 \\u0645\\u0647 \\u063a\\u0644\\u06cc\\u0638\\u06cc \\u062f\\u0631 \\u0645\\u062d\\u06cc\\u0637 \\u067e\\u06cc\\u0686\\u06cc\\u062f\\u0647 \\u0628\\u0648\\u062f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"phoneme\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 400,\n        \"samples\": [\n          \"?AyA bAyad haqiqat rA be ?AnhA beguyim\\u061f\",\n          \"ke piS ?az ?enqelAb be xAbgAh-e doxtarAn va zanAn-e nAbinA ?extesAsyAfte bud ?aqlab-e zanAni ke dar ?in xAbgAh zendegi mikardand\",\n          \"dud-o meh-e qalizi dar mohit piCide bud\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"homograph word\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 101,\n        \"samples\": [\n          \"\\u06af\\u0631\\u06cc\\u0645\",\n          \"\\u0633\\u0628\\u06a9\\u06cc\",\n          \"\\u06a9\\u0645\\u06cc\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pronunciation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 210,\n        \"samples\": [\n          \"darham\",\n          \"Sum\",\n          \"moSk\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "sentence_bench.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDV7ysXf2b_H"
      },
      "source": [
        "### Get ManaTTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcL5ZLvSSnVB",
        "outputId": "076c85f5-4317-4a47-f245-1e58576fcc13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Ø¯Ø± Ø§ÛŒÙ† Ù†ÙˆØ´ØªÙ‡ Ø¨Ù†Ø§ Ø¯Ø§Ø±ÛŒÙ… Ø¨Ø§ ÛŒÚ© Ø§Ø¨Ø²Ø§Ø± Ø³Ø§Ø¯Ù‡ Ùˆ Ù…Ú©Ø§Ù†ÛŒÚ©ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ø¨ÛŒÙ†Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÙØ±Ø§Ø¯ Ú©Ù…\\u200cØ¨ÛŒÙ†Ø§ ',\n",
              "  'dar ?in neveSte banA dArim bA yek ?abzAr-e sAde va mekAniki-ye ?afzAyeS-e binAyi barAye ?afrAd-e kam\\u200cbinA '),\n",
              " ('Ø¨Ù‡ Ù†Ø§Ù… Ø¨ÛŒ\\u200cÙˆÙ¾ØªÛŒÚ© ÛŒØ§ Ø¹Ø¯Ø³ÛŒ Ø¯ÙˆØ±Ù†Ù…Ø§ Ø¢Ø´Ù†Ø§ Ø´ÙˆÛŒÙ…. ',\n",
              "  'be nAm-e biyoptik yA ?adasi-ye durnamA ?ASnA Savim'),\n",
              " ('Ø¯Ø±Ø§ÛŒÙ†\\u200cØµÙˆØ±ØªØŒ Ø§Ù†Ø¬Ø§Ù… Ø®ÙˆØ¯Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ùˆ Ø§Ø±Ø§Ø¦Ù‡ Ø¨Ø§Ø²Ø®ÙˆØ±Ø¯ Ø¨Ø± Ø¹Ù‡Ø¯Ù‡ Ø®ÙˆØ¯ØªØ§Ù† Ø§Ø³Øª. ',\n",
              "  'dar ?in surat ?anjAm-e xod?arzyAbi va ?erA?e-ye bAzxord bar ?ohde-ye xodetAn ?ast ')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "filtered_rows = sentence_bench[sentence_bench['dataset'] == 'mana-tts'][['grapheme', 'phoneme']]\n",
        "\n",
        "# Convert to a list of tuples\n",
        "mana_evaluation_data = list(filtered_rows.itertuples(index=False, name=None))\n",
        "\n",
        "mana_evaluation_data[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjacw9Mp2eoX"
      },
      "source": [
        "### Get CommonVoice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yQnqCGw26sk",
        "outputId": "6230d37a-0948-42f2-895c-b89c8991316c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Ø¯Ø± Ø§Ú©Ø«Ø± Ø´Ù‡Ø±Ù‡Ø§ØŒ Ù…Ø±Ú©Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø®Ø±ÛŒØ¯ Ø¯ÙˆÚ†Ø±Ø®Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯.',\n",
              "  'dar ?aksar-e Sahr-hA, markazi barAye xarid-e  doCarxe vojud dArad.'),\n",
              " ('Ù¾Ø³ Ø§Ø² Ù…Ø¯Ø±Ø³Ù‡ Ú©ÙˆØ¯Ú©Ø§Ù† Ø¨Ù‡ Ø³ÙˆÛŒ Ø®Ø§Ù†Ù‡ Ø¬Ø³Øª Ùˆ Ø®ÛŒØ² Ú©Ø±Ø¯Ù†Ø¯.',\n",
              "  'pas ?az madrese kudakAn be suye xAne jast-o-xiz kardand.'),\n",
              " ('Ø´Ù…Ø§ Ù†Ú¯Ø±Ø§Ù† Ø²Ù† Ùˆ Ø¨Ú†Ù‡ Ø§ÛŒÙ† Ù†Ø¨Ø§Ø´.', 'SomA negarAn-e zan-o-baCCe-ye ?in nabAS.')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "filtered_rows = sentence_bench[sentence_bench['dataset'] == 'commonvoice'][['grapheme', 'phoneme']]\n",
        "\n",
        "# Convert to a list of tuples\n",
        "commonvoice_evaluation_data = list(filtered_rows.itertuples(index=False, name=None))\n",
        "\n",
        "commonvoice_evaluation_data[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciSPyhRc3Rvo"
      },
      "source": [
        "### Get Homograph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlFc5JbN3Rvz",
        "outputId": "c84d81e9-a206-4d4e-eaca-9e3a9d29dc71"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Ù…Ù† Ù‚Ø¯Ø± ØªÙˆ Ø±Ø§ Ù…ÛŒ\\u200cØ¯Ø§Ù†Ù…', 'man qadr-e to rA mi-dAnam', 'Ù‚Ø¯Ø±', 'qadr'),\n",
              " ('Ø§Ø² Ù‚Ø¶Ø§ÛŒ Ø§Ù„Ù‡ÛŒ Ø¨Ù‡ Ù‚Ø¯Ø± Ø§Ù„Ù‡ÛŒ Ù¾Ù†Ø§Ù‡ Ù…ÛŒ\\u200cØ¨Ø±Ù…',\n",
              "  '?az qazAy ?elAhi be qadar-e ?elAhi panAh mi-baram',\n",
              "  'Ù‚Ø¯Ø±',\n",
              "  'qadar'),\n",
              " ('Ø¨Ù‡ Ø¯Ø³Øª Ùˆ ØµÙˆØ±ØªÙ… Ú©Ø±Ù… Ø²Ø¯Ù…', 'be dast-o suratam kerem zadam', 'Ú©Ø±Ù…', 'kerem')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "filtered_rows = sentence_bench[sentence_bench['dataset'] == 'homograph'][['grapheme', 'phoneme', 'homograph word',\t'pronunciation']]\n",
        "\n",
        "# Convert to a list of tuples\n",
        "homograph_evaluation_data = list(filtered_rows.itertuples(index=False, name=None))\n",
        "\n",
        "homograph_evaluation_data[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGEUIrbi9kNH"
      },
      "source": [
        "# Full bench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGzQvL8V9mln"
      },
      "outputs": [],
      "source": [
        "benchmark = []\n",
        "\n",
        "for g, p in mana_evaluation_data:\n",
        "  benchmark.append((g, p, '', ''))\n",
        "\n",
        "for g, p in commonvoice_evaluation_data:\n",
        "  benchmark.append((g, p, '', ''))\n",
        "\n",
        "for g, p, w, r in homograph_evaluation_data:\n",
        "  benchmark.append((g, p, w, r))\n",
        "\n",
        "benchmark = benchmark[:400]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jlXFt8tCPWB"
      },
      "outputs": [],
      "source": [
        "def print_all_metrics(predictions):\n",
        "  per = get_avg_cer_of_method(predictions, benchmark) * 100\n",
        "  # acc, prec, recall = get_phonetic_model_performance(predictions, benchmark)\n",
        "  homograph = get_homograph_performance(predictions, benchmark) * 100\n",
        "\n",
        "  print(f\"PER: \\t\\t\\t{per:.4f}\")\n",
        "  print(f\"HOMOGRAPH: \\t\\t{homograph:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTRgGM_8_Fwg"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajqTWtNb_HBd"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "outputs = [predict_ezafe_simple(item[0], model, tokenizer) for item in benchmark]\n",
        "\n",
        "total_time = time.time() - start_time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_time = total_time / len(outputs) if len(outputs) > 0 else 0\n",
        "print(f\"AVG TIME:\\t\\t{avg_time:.4f} (s)+\")"
      ],
      "metadata": {
        "id": "muSFPxoi5wvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31ea66fd-203a-4a8e-db01-f7c6629f8020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG TIME:\t\t0.0368 (s)+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runs\n",
        "\n",
        "## First\n",
        "```\n",
        "True Positives (TP): 519\n",
        "True Negatives (TN): 2451\n",
        "False Positives (FP): 33\n",
        "False Negatives (FN): 31\n",
        "Total tokens: 3034\n",
        "Precision: 0.9402\n",
        "Recall: 0.9436\n",
        "F1 Score: 0.9419\n",
        "{'TP': 519,\n",
        " 'TN': 2451,\n",
        " 'FP': 33,\n",
        " 'FN': 31,\n",
        " 'Precision': 0.9402173913043478,\n",
        " 'Recall': 0.9436363636363636,\n",
        " 'F1': 0.9419237749546279}\n",
        "\n",
        "AVG TIME:\t\t0.0384 (s)+\n",
        "```\n",
        "\n",
        "\n",
        "## Second\n",
        "```\n",
        "True Positives (TP): 519\n",
        "True Negatives (TN): 2451\n",
        "False Positives (FP): 33\n",
        "False Negatives (FN): 31\n",
        "Total tokens: 3034\n",
        "Precision: 0.9402\n",
        "Recall: 0.9436\n",
        "F1 Score: 0.9419\n",
        "{'TP': 519,\n",
        " 'TN': 2451,\n",
        " 'FP': 33,\n",
        " 'FN': 31,\n",
        " 'Precision': 0.9402173913043478,\n",
        " 'Recall': 0.9436363636363636,\n",
        " 'F1': 0.9419237749546279}\n",
        "\n",
        "\n",
        "AVG TIME:\t\t0.0370 (s)+\n",
        "```\n",
        "\n",
        "\n",
        "## Third\n",
        "```\n",
        "True Positives (TP): 519\n",
        "True Negatives (TN): 2451\n",
        "False Positives (FP): 33\n",
        "False Negatives (FN): 31\n",
        "Total tokens: 3034\n",
        "Precision: 0.9402\n",
        "Recall: 0.9436\n",
        "F1 Score: 0.9419\n",
        "{'TP': 519,\n",
        " 'TN': 2451,\n",
        " 'FP': 33,\n",
        " 'FN': 31,\n",
        " 'Precision': 0.9402173913043478,\n",
        " 'Recall': 0.9436363636363636,\n",
        " 'F1': 0.9419237749546279}\n",
        "\n",
        "\n",
        "AVG TIME:\t\t0.0368 (s)+\n",
        "```\n"
      ],
      "metadata": {
        "id": "tDI7lQMr10JV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# === Your three runs ===\n",
        "runs = [\n",
        "    {\n",
        "        \"TP\": 519,\n",
        "        \"TN\": 2451,\n",
        "        \"FP\": 33,\n",
        "        \"FN\": 31,\n",
        "        \"Precision\": 0.9402173913043478,\n",
        "        \"Recall\": 0.9436363636363636,\n",
        "        \"F1\": 0.9419237749546279,\n",
        "        \"AVG_TIME\": 0.0384,\n",
        "    },\n",
        "    {\n",
        "        \"TP\": 519,\n",
        "        \"TN\": 2451,\n",
        "        \"FP\": 33,\n",
        "        \"FN\": 31,\n",
        "        \"Precision\": 0.9402173913043478,\n",
        "        \"Recall\": 0.9436363636363636,\n",
        "        \"F1\": 0.9419237749546279,\n",
        "        \"AVG_TIME\": 0.0370,\n",
        "    },\n",
        "    {\n",
        "        \"TP\": 519,\n",
        "        \"TN\": 2451,\n",
        "        \"FP\": 33,\n",
        "        \"FN\": 31,\n",
        "        \"Precision\": 0.9402173913043478,\n",
        "        \"Recall\": 0.9436363636363636,\n",
        "        \"F1\": 0.9419237749546279,\n",
        "        \"AVG_TIME\": 0.0368,\n",
        "    },\n",
        "]\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(runs)\n",
        "\n",
        "# === Compute Accuracy ===\n",
        "df[\"Accuracy\"] = (df[\"TP\"] + df[\"TN\"]) / (df[\"TP\"] + df[\"TN\"] + df[\"FP\"] + df[\"FN\"])\n",
        "\n",
        "# === Compute mean and std ===\n",
        "mean_vals = df.mean()\n",
        "std_vals = df.std(ddof=1)\n",
        "\n",
        "# === Print report ===\n",
        "print(\"=== Ezafe Detection Evaluation Report (predict_ezafe_simple) ===\\n\")\n",
        "print(\"Averaged over 3 runs\\n\")\n",
        "\n",
        "# Define order of metrics\n",
        "metrics_order = [\"TP\", \"TN\", \"FP\", \"FN\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AVG_TIME\"]\n",
        "\n",
        "for metric in metrics_order:\n",
        "    mean_ = mean_vals[metric]\n",
        "    std_ = std_vals[metric]\n",
        "\n",
        "    if metric in [\"Precision\", \"Recall\", \"F1\", \"Accuracy\"]:\n",
        "        print(f\"{metric:10s}: {mean_:.4f} Â± {std_:.4f}\")\n",
        "    elif metric == \"AVG_TIME\":\n",
        "        print(f\"{metric:10s}: {mean_:.4f} Â± {std_:.4f}  (seconds)\")\n",
        "    else:\n",
        "        print(f\"{metric:10s}: {mean_:.0f} Â± {std_:.0f}\")"
      ],
      "metadata": {
        "id": "iqRMNJ0BMlYh",
        "outputId": "5cc4ad0f-f542-4f3e-d28a-326be2405dc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Ezafe Detection Evaluation Report (predict_ezafe_simple) ===\n",
            "\n",
            "Averaged over 3 runs\n",
            "\n",
            "TP        : 519 Â± 0\n",
            "TN        : 2451 Â± 0\n",
            "FP        : 33 Â± 0\n",
            "FN        : 31 Â± 0\n",
            "Accuracy  : 0.9789 Â± 0.0000\n",
            "Precision : 0.9402 Â± 0.0000\n",
            "Recall    : 0.9436 Â± 0.0000\n",
            "F1        : 0.9419 Â± 0.0000\n",
            "AVG_TIME  : 0.0374 Â± 0.0009  (seconds)\n"
          ]
        }
      ]
    }
  ]
}