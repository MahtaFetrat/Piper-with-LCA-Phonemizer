{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "DvyzQ8vI8X_w",
        "0ZXR4gz3JKeU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install piper-phonemize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvzgiirCF5mS",
        "outputId": "88115561-bb0a-40a1-a57d-15a8ee9d6fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement piper-phonemize (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for piper-phonemize\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/roshan-research/spacy_pos_tagger_parsbertpostagger"
      ],
      "metadata": {
        "id": "ZXRUGaQ_X4AN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e59dd03-d0ba-4205-a068-3b0f3ff24471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'spacy_pos_tagger_parsbertpostagger'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 21 (delta 0), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (21/21), 719.37 KiB | 1.15 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/pos_tagger.py https://raw.githubusercontent.com/roshan-research/hazm/refs/heads/master/hazm/pos_tagger.py\n",
        "!echo -e \"import spacy\\n\\nfrom spacy.tokens import Doc\\nfrom spacy.tokens import DocBin\\nfrom spacy.vocab import Vocab\\n\\nfrom sklearn.metrics import classification_report,f1_score,accuracy_score,precision_score,recall_score\\n\\nfrom tqdm import tqdm\\n\" > /tmp/new_file.py && cat /content/pos_tagger.py >> /tmp/new_file.py && mv /tmp/new_file.py /content/pos_tagger.py\n",
        "!sed -i \"s/self\\.tagger(\\[' '\\.join(\\[tok for tok in tokens\\])\\])/self.tagger(' '.join(tokens))/g\" /content/pos_tagger.py"
      ],
      "metadata": {
        "id": "EhCQKgqgiWNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a619258-ee4c-4cb9-88ef-be5dc54fd311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-20 07:23:39--  https://raw.githubusercontent.com/roshan-research/hazm/refs/heads/master/hazm/pos_tagger.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28649 (28K) [text/plain]\n",
            "Saving to: ‘/content/pos_tagger.py’\n",
            "\n",
            "\r/content/pos_tagger   0%[                    ]       0  --.-KB/s               \r/content/pos_tagger 100%[===================>]  27.98K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2025-08-20 07:23:40 (13.0 MB/s) - ‘/content/pos_tagger.py’ saved [28649/28649]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/roshan-research/hazm.git\n",
        "\n",
        "# Replace Python version constraint\n",
        "!sed -i 's/python= \">=3.8, <3.12\"/python= \">=3.8\"/g' hazm/pyproject.toml\n",
        "\n",
        "# Replace numpy version constraint\n",
        "!sed -i 's/numpy = \"~1.24\"/numpy = \"1.26\"/g' hazm/pyproject.toml\n",
        "\n",
        "!pip install ./hazm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eRXOurFhXOzX",
        "outputId": "76ef8706-69bb-44df-f659-9c514b21e9a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hazm'...\n",
            "remote: Enumerating objects: 8674, done.\u001b[K\n",
            "remote: Counting objects: 100% (1801/1801), done.\u001b[K\n",
            "remote: Compressing objects: 100% (303/303), done.\u001b[K\n",
            "remote: Total 8674 (delta 1651), reused 1498 (delta 1498), pack-reused 6873 (from 3)\u001b[K\n",
            "Receiving objects: 100% (8674/8674), 24.46 MiB | 19.46 MiB/s, done.\n",
            "Resolving deltas: 100% (5548/5548), done.\n",
            "Processing ./hazm\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fasttext-wheel<0.10.0,>=0.9.2 (from hazm==0.10.0)\n",
            "  Using cached fasttext_wheel-0.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting flashtext<3.0,>=2.7 (from hazm==0.10.0)\n",
            "  Using cached flashtext-2.7.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gensim<5.0.0,>=4.3.1 (from hazm==0.10.0)\n",
            "  Using cached gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.12/dist-packages (from hazm==0.10.0) (3.9.1)\n",
            "Collecting numpy==1.26 (from hazm==0.10.0)\n",
            "  Using cached numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Collecting python-crfsuite<0.10.0,>=0.9.9 (from hazm==0.10.0)\n",
            "  Using cached python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from hazm==0.10.0) (1.6.1)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel<0.10.0,>=0.9.2->hazm==0.10.0)\n",
            "  Downloading pybind11-3.0.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm==0.10.0) (75.2.0)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim<5.0.0,>=4.3.1->hazm==0.10.0)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim<5.0.0,>=4.3.1->hazm==0.10.0) (7.3.0.post1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk<4.0.0,>=3.8.1->hazm==0.10.0) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk<4.0.0,>=3.8.1->hazm==0.10.0) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk<4.0.0,>=3.8.1->hazm==0.10.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk<4.0.0,>=3.8.1->hazm==0.10.0) (4.67.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm==0.10.0) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm==0.10.0) (1.17.3)\n",
            "Downloading numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasttext_wheel-0.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-3.0.0-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.1/292.1 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: hazm, flashtext\n",
            "  Building wheel for hazm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hazm: filename=hazm-0.10.0-py3-none-any.whl size=893958 sha256=56968152514fe6dbf17ff14dc259f527190ae1ff5b0ad776fb503f3dbadd0651\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3s5ng1cc/wheels/bc/2f/5d/575b4342ef263c10935d0176afc20405e8ffbdef19d48a9cf8\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9300 sha256=055f6d71974e9fd98f713eb4ab7f6f19c6af22e04929a6b8f1a4842b53b695c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/24/da/4d994d7a27cfc73a4e513a669fbeec4a71f871fe245a81977f\n",
            "Successfully built hazm flashtext\n",
            "Installing collected packages: flashtext, python-crfsuite, pybind11, numpy, scipy, fasttext-wheel, gensim, hazm\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fasttext-wheel-0.9.2 flashtext-2.7 gensim-4.3.3 hazm-0.10.0 numpy-1.26.0 pybind11-3.0.0 python-crfsuite-0.9.11 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "hazm",
                  "numpy",
                  "scipy"
                ]
              },
              "id": "826220ad2c6743c0b9ba9bed2d78e096"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U  spacy[cuda12x] spacy-transformers numpy==1.26.4"
      ],
      "metadata": {
        "id": "l3r2JD0jRXV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b6aaff8-23bd-4a03-dec6-c92c0ae0eff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (1.18.0)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.12/dist-packages (1.22.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.49.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-transformers in /usr/local/lib/python3.12/dist-packages (1.3.9)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: optimum[exporters] in /usr/local/lib/python3.12/dist-packages (1.27.0)\n",
            "Requirement already satisfied: spacy[cuda12x] in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.12/dist-packages (from optimum[exporters]) (2.8.0+cu126)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from optimum[exporters]) (25.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from optimum[exporters]) (0.34.4)\n",
            "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.12/dist-packages (from optimum[exporters]) (5.29.5)\n",
            "  Using cached transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.14.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy[cuda12x]) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy[cuda12x]) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy[cuda12x]) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy[cuda12x]) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy[cuda12x]) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy[cuda12x]) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy[cuda12x]) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy[cuda12x]) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy[cuda12x]) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy[cuda12x]) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy[cuda12x]) (0.16.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy[cuda12x]) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy[cuda12x]) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy[cuda12x]) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy[cuda12x]) (3.5.0)\n",
            "Requirement already satisfied: cupy-cuda12x<13.0.0,>=11.5.0 in /usr/local/lib/python3.12/dist-packages (from spacy[cuda12x]) (12.3.0)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from spacy-transformers) (0.9.2)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x<13.0.0,>=11.5.0->spacy[cuda12x]) (0.8.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[exporters]) (1.1.7)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy[cuda12x]) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy[cuda12x]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy[cuda12x]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy[cuda12x]) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy[cuda12x]) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy[cuda12x]) (0.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[exporters]) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[exporters]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[exporters]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[exporters]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[exporters]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[exporters]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[exporters]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[exporters]) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[exporters]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[exporters]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[exporters]) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[exporters]) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[exporters]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[exporters]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[exporters]) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[exporters]) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy[cuda12x]) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy[cuda12x]) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy[cuda12x]) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy[cuda12x]) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy[cuda12x]) (7.3.0.post1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy[cuda12x]) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy[cuda12x]) (1.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy[cuda12x]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy[cuda12x]) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy[cuda12x]) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy[cuda12x]) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForTokenClassification,\n",
        "    TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
        ")\n",
        "import openai\n",
        "from sklearn.metrics import classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "D7PD4qFaWKQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "WtiRO0PyWMwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "lSkc3E4OWOHq",
        "outputId": "636f62ba-43d2-4170-82f3-9db83211fef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "from typing import List\n",
        "\n",
        "class PersianTextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.arabic_to_persian = {\n",
        "            'ك': 'ک',\n",
        "            'ي': 'ی',\n",
        "            'ء': '',\n",
        "            'أ': 'ا',\n",
        "            'إ': 'ا',\n",
        "            'آ': 'ا',\n",
        "            'ة': 'ه',\n",
        "            'ؤ': 'و',\n",
        "            'ئ': 'ی',\n",
        "        }\n",
        "\n",
        "        self.persian_digits = '۰۱۲۳۴۵۶۷۸۹'\n",
        "        self.arabic_digits = '٠١٢٣٤٥٦٧٨٩'\n",
        "        self.english_digits = '0123456789'\n",
        "\n",
        "        self.diacritics = 'ًٌٍَُِّْٰٕٖٜٟٔٗ٘ٙٚٛٝٞٱٲٳٴٵٶٷٹٺٻټٽٿ'\n",
        "\n",
        "    def normalize_text(self, text: str) -> str:\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        for arabic, persian in self.arabic_to_persian.items():\n",
        "            text = text.replace(arabic, persian)\n",
        "\n",
        "        for diacritic in self.diacritics:\n",
        "            text = text.replace(diacritic, '')\n",
        "\n",
        "        for persian_digit, english_digit in zip(self.persian_digits, self.english_digits):\n",
        "            text = text.replace(persian_digit, english_digit)\n",
        "\n",
        "        for arabic_digit, english_digit in zip(self.arabic_digits, self.english_digits):\n",
        "            text = text.replace(arabic_digit, english_digit)\n",
        "\n",
        "        text = text.replace('\\u200c', '')\n",
        "        text = text.replace('\\u200d', '')\n",
        "        text = text.replace('\\u200e', '')\n",
        "        text = text.replace('\\u200f', '')\n",
        "\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "\n",
        "        text = re.sub(r'[۔\\.]{2,}', '.', text)\n",
        "        text = re.sub(r'[؟\\?]{2,}', '؟', text)\n",
        "        text = re.sub(r'[!]{2,}', '!', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def tokenize_words(self, text: str) -> List[str]:\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        text = self.normalize_text(text)\n",
        "\n",
        "        persian_punctuation = '۔؟!،؍؎؏ؘؙؚؐؑؒؓؔؕؖؗ؛؜؝؞؟'\n",
        "        english_punctuation = '.,;:!?\"\\'()[]{}«»\"\"''…-–—'\n",
        "        all_punctuation = persian_punctuation + english_punctuation\n",
        "\n",
        "        pattern = f'([{re.escape(all_punctuation)}\\\\s]+)'\n",
        "\n",
        "        tokens = re.split(pattern, text)\n",
        "\n",
        "        words = []\n",
        "        for token in tokens:\n",
        "            token = token.strip()\n",
        "            if token and not all(c in all_punctuation + ' \\t\\n\\r' for c in token):\n",
        "                if re.search(r'[\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\w]', token):\n",
        "                    words.append(token)\n",
        "\n",
        "        return words"
      ],
      "metadata": {
        "id": "vetscsdXV6-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## open ai dataset"
      ],
      "metadata": {
        "id": "DvyzQ8vI8X_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class PersianDatasetLoader:\n",
        "#     def __init__(self):\n",
        "#         self.preprocessor = PersianTextPreprocessor()\n",
        "\n",
        "#     def load_persian_datasets(self, max_sentences: int = 200) -> List[str]:\n",
        "#         sentences = self._get_sample_persian_sentences()\n",
        "#         sentences = sentences * (max_sentences // len(sentences) + 1)\n",
        "#         random.shuffle(sentences)\n",
        "\n",
        "#         normalized_sentences = []\n",
        "#         for sent in sentences:\n",
        "#             normalized = self.preprocessor.normalize_text(sent)\n",
        "#             if 4 <= len(normalized) <= 150:\n",
        "#                 normalized_sentences.append(normalized)\n",
        "\n",
        "#         if len(normalized_sentences) > max_sentences:\n",
        "#             normalized_sentences = random.sample(normalized_sentences, max_sentences)\n",
        "\n",
        "#         return normalized_sentences[:max_sentences]\n",
        "\n",
        "#     def _get_sample_persian_sentences(self) -> List[str]:\n",
        "#         return [\n",
        "#             # Basic greetings and conversations\n",
        "#             \"سلام، چطور هستید؟\",\n",
        "#             \"من خوب هستم، ممنون.\",\n",
        "#             \"شما چه کار می‌کنید؟\",\n",
        "#             \"من در یک شرکت کار می‌کنم.\",\n",
        "#             \"خوشحالم که شما را ملاقات کردم.\",\n",
        "#             \"روز شما چطور گذشت؟\",\n",
        "#             \"حال شما چطور است؟\",\n",
        "#             \"کجا زندگی می‌کنید؟\",\n",
        "#             \"با شما آشنا شدم خوشحال شدم.\",\n",
        "#             \"امیدوارم روز خوبی داشته باشید.\",\n",
        "\n",
        "#             # Daily activities\n",
        "#             \"امروز هوا بسیار زیبا است.\",\n",
        "#             \"من به دانشگاه می‌روم.\",\n",
        "#             \"فردا به سینما خواهیم رفت.\",\n",
        "#             \"دوستان من در پارک بازی می‌کنند.\",\n",
        "#             \"مادرم شیرینی پخت.\",\n",
        "#             \"صبحانه مهم‌ترین وعده غذایی روز است.\",\n",
        "#             \"بچه‌ها در حیاط مدرسه بازی می‌کنند.\",\n",
        "#             \"کودکان با شادی بازی می‌کنند.\",\n",
        "#             \"باران پاییزی هوای تازه‌ای می‌آورد.\",\n",
        "#             \"من هر روز صبح ورزش می‌کنم.\",\n",
        "#             \"شام را با خانواده می‌خوریم.\",\n",
        "#             \"کتاب خواندن عادت روزانه من است.\",\n",
        "#             \"قدم زدن در پارک آرام‌بخش است.\",\n",
        "#             \"تلویزیون تماشا کردن تفریح من است.\",\n",
        "#             \"موسیقی گوش دادن آرامم می‌کند.\",\n",
        "\n",
        "#             # Education and learning\n",
        "#             \"استاد درس زیبایی تدریس کرد.\",\n",
        "#             \"کتابخانه مکان آرامی برای مطالعه است.\",\n",
        "#             \"دانشجویان با علاقه درس می‌خوانند.\",\n",
        "#             \"کودکان در مدرسه چیزهای جدید یاد می‌گیرند.\",\n",
        "#             \"استادان دانشگاه تجربه زیادی دارند.\",\n",
        "#             \"کتاب‌های تاریخی داستان‌های جالبی دارند.\",\n",
        "#             \"محققان ایرانی در علوم مختلف فعال هستند.\",\n",
        "#             \"درس ریاضی برای من سخت است.\",\n",
        "#             \"زبان انگلیسی یادگیری مفید است.\",\n",
        "#             \"امتحان فردا خیلی مهم است.\",\n",
        "#             \"تکالیف مدرسه را باید انجام دهم.\",\n",
        "#             \"مطالعه در کتابخانه بهتر است.\",\n",
        "#             \"معلم ما بسیار صبور است.\",\n",
        "#             \"دانش آموزان باید سخت درس بخوانند.\",\n",
        "#             \"آزمون ورودی دانشگاه سخت است.\",\n",
        "\n",
        "#             # Culture and literature\n",
        "#             \"کتاب‌های فارسی بسیار جالب هستند.\",\n",
        "#             \"شعرهای حافظ بسیار زیبا هستند.\",\n",
        "#             \"زبان فارسی زبان شعر و ادب است.\",\n",
        "#             \"موسیقی سنتی ایران تاریخ غنی دارد.\",\n",
        "#             \"هنرمندان ایرانی در سراسر جهان مشهور هستند.\",\n",
        "#             \"فرهنگ غنی ایران در تمام جهان شناخته شده است.\",\n",
        "#             \"شعرهای فردوسی حماسی هستند.\",\n",
        "#             \"سعدی و حافظ شاعران بزرگ ایران هستند.\",\n",
        "#             \"شاهنامه شاهکار ادبیات فارسی است.\",\n",
        "#             \"موسیقی کلاسیک ایران عمیق است.\",\n",
        "#             \"نقاشی مینیاتور هنر ایرانی است.\",\n",
        "#             \"خوشنویسی فارسی بسیار زیبا است.\",\n",
        "#             \"تعزیه نمایش سنتی ایرانی است.\",\n",
        "#             \"فرش ایرانی شاهکار هنر است.\",\n",
        "#             \"معماری اسلامی در ایران شکوفا شده است.\",\n",
        "\n",
        "#             # Geography and places\n",
        "#             \"تهران پایتخت ایران است.\",\n",
        "#             \"رودخانه کارون طولانی‌ترین رود ایران است.\",\n",
        "#             \"کوه‌های البرز منظره‌ای فوق‌العاده دارند.\",\n",
        "#             \"باغ‌های شیراز بسیار معروف هستند.\",\n",
        "#             \"دریای خزر در شمال ایران قرار دارد.\",\n",
        "#             \"شهرهای تاریخی ایران جاذبه‌های توریستی هستند.\",\n",
        "#             \"اصفهان نصف جهان نامیده می‌شود.\",\n",
        "#             \"شیراز شهر شعر و ادب ایران است.\",\n",
        "#             \"مشهد شهر مقدس ایران است.\",\n",
        "#             \"تبریز شهر فرش‌های زیبا است.\",\n",
        "#             \"کاشان شهر گلاب است.\",\n",
        "#             \"یزد شهر بادگیرها است.\",\n",
        "#             \"کرمان شهر پسته است.\",\n",
        "#             \"گیلان استان برنج است.\",\n",
        "#             \"فارس استان تاریخی ایران است.\",\n",
        "\n",
        "#             # Food and hospitality\n",
        "#             \"غذای ایرانی خیلی خوشمزه است.\",\n",
        "#             \"چلو کباب غذای محبوب ایرانی است.\",\n",
        "#             \"چای ایرانی طعم خاصی دارد.\",\n",
        "#             \"خانواده‌های ایرانی بسیار مهمان‌نواز هستند.\",\n",
        "#             \"مردم ایران بسیار مهربان و صمیمی هستند.\",\n",
        "#             \"قورمه سبزی غذای خوشمزه‌ای است.\",\n",
        "#             \"فسنجان غذای شمالی ایران است.\",\n",
        "#             \"آش رشته غذای محبوب زمستانی است.\",\n",
        "#             \"کوکو سبزی غذای بهاری است.\",\n",
        "#             \"باقلا پلو با محوه غذای گیلانی است.\",\n",
        "#             \"خورشت بامیه غذای خانگی است.\",\n",
        "#             \"دوغ نوشیدنی سنتی ایران است.\",\n",
        "#             \"شیرینی یزدی بسیار خوشمزه است.\",\n",
        "#             \"بستنی سنتی شیراز معروف است.\",\n",
        "#             \"حلوا شیرینی محبوب ایرانی است.\",\n",
        "#         ]"
      ],
      "metadata": {
        "id": "mnIGv3QUXdBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OPENAI_API_KEY = \"\""
      ],
      "metadata": {
        "id": "E7fSVQY1WGLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import openai\n",
        "# import json\n",
        "# from typing import Dict, List\n",
        "# from pydantic import BaseModel\n",
        "\n",
        "# class EzafeLabels(BaseModel):\n",
        "#     needs_ezafe: List[bool]\n",
        "\n",
        "# class OpenAIEzafeLabeler:\n",
        "#     def __init__(self, api_key: str):\n",
        "#         self.client = openai.OpenAI(api_key=api_key)\n",
        "#         self.preprocessor = PersianTextPreprocessor()\n",
        "\n",
        "#     def label_sentence(self, sentence: str) -> Dict:\n",
        "#         words = self.preprocessor.tokenize_words(sentence)\n",
        "\n",
        "#         system_prompt = f\"\"\"\n",
        "# تو یک متخصص زبان فارسی هستی. فهرست کلمات زیر را تحلیل کن و مشخص کن کدام کلمات نیاز به \"اِ ربط\" دارند.\n",
        "\n",
        "# قوانین:\n",
        "# - اِ ربط (کسره): بین صفت و موصوف، مضاف و مضاف‌الیه استفاده می‌شود\n",
        "# - فقط کلماتی که به کلمه بعدی متصل می‌شوند نیاز به اِ ربط دارند\n",
        "# - آخرین کلمه هیچ‌وقت نیاز به اِ ربط ندارد\n",
        "# - باید دقیقاً به تعداد کلمات ورودی، جواب true/false بدهی\n",
        "\n",
        "# مثال‌ها:\n",
        "\n",
        "# مثال 1:\n",
        "# کلمات: [\"کتاب\", \"قرمز\", \"دانشجو\"]\n",
        "# needs_ezafe: [true, true, false]\n",
        "# توضیح: \"کتاب\" نیاز به اِ ربط دارد (کتابِ قرمز)، \"قرمز\" نیاز به اِ ربط دارد (قرمزِ دانشجو)، \"دانشجو\" نیاز ندارد.\n",
        "\n",
        "# مثال 2:\n",
        "# کلمات: [\"خانه\", \"بزرگ\", \"مادربزرگ\", \"من\"]\n",
        "# needs_ezafe: [true, true, true, false]\n",
        "# توضیح: \"خانه\" نیاز دارد (خانه‌ی بزرگ)، \"بزرگ\" نیاز دارد (بزرگِ مادربزرگ)، \"مادربزرگ\" نیاز دارد (مادربزرگِ من)، \"من\" نیاز ندارد.\n",
        "\n",
        "# مثال 3:\n",
        "# کلمات: [\"دختر\", \"زیبا\", \"خواب\", \"است\"]\n",
        "# needs_ezafe: [true, false, false, false]\n",
        "# توضیح: \"دختر\" نیاز دارد (دخترِ زیبا)، بقیه نیاز ندارند.\n",
        "\n",
        "# مثال 4:\n",
        "# کلمات: [\"درس\", \"ریاضی\", \"سخت\", \"است\"]\n",
        "# needs_ezafe: [true, false, false, false]\n",
        "# توضیح: \"درس\" نیاز دارد (درسِ ریاضی)، بقیه نیاز ندارند.\n",
        "\n",
        "# مثال 5:\n",
        "# کلمات: [\"کتابخانه\", \"دانشگاه\", \"تهران\", \"بزرگ\", \"است\"]\n",
        "# needs_ezafe: [true, true, false, false, false]\n",
        "# توضیح: \"کتابخانه\" نیاز دارد (کتابخانه‌ی دانشگاه)، \"دانشگاه\" نیاز دارد (دانشگاهِ تهران)، بقیه نیاز ندارند.\n",
        "\n",
        "# مثال 6:\n",
        "# کلمات: [\"شعرهای\", \"حافظ\", \"زیبا\", \"هستند\"]\n",
        "# needs_ezafe: [true, false, false, false]\n",
        "# توضیح: \"شعرهای\" نیاز دارد (شعرهایِ حافظ)، بقیه نیاز ندارند.\n",
        "\n",
        "# مثال 7:\n",
        "# کلمات: [\"استاد\", \"درس\", \"زیبایی\", \"تدریس\", \"کرد\"]\n",
        "# needs_ezafe: [false, true, false, false, false]\n",
        "# توضیح: \"درس\" نیاز دارد (درسِ زیبایی)، بقیه نیاز ندارند.\n",
        "\n",
        "# مثال 8:\n",
        "# کلمات: [\"موسیقی\", \"سنتی\", \"ایران\", \"زیبا\", \"است\"]\n",
        "# needs_ezafe: [true, true, false, false, false]\n",
        "# توضیح: \"موسیقی\" نیاز دارد (موسیقیِ سنتی)، \"سنتی\" نیاز دارد (سنتیِ ایران)، بقیه نیاز ندارند.\n",
        "\n",
        "# حالا تعداد کلمات ورودی: {len(words)}\n",
        "# باید دقیقاً {len(words)} مقدار true/false برگردانی.\n",
        "# \"\"\"\n",
        "\n",
        "#         words_str = str(words)\n",
        "\n",
        "#         response = self.client.beta.chat.completions.parse(\n",
        "#             model=\"gpt-4.1\",\n",
        "#             messages=[\n",
        "#                 {\"role\": \"system\", \"content\": system_prompt},\n",
        "#                 {\"role\": \"user\", \"content\": f\"جمله: {sentence}\\nکلمات: {words_str}\"}\n",
        "#             ],\n",
        "#             response_format=EzafeLabels,\n",
        "#             temperature=0.1,\n",
        "#             max_tokens=500\n",
        "#         )\n",
        "\n",
        "#         labels = response.choices[0].message.parsed.needs_ezafe\n",
        "\n",
        "#         if len(labels) != len(words):\n",
        "#             raise Exception(f\"Mismatch in length! Words: {len(words)}, Labels: {len(labels)}\")\n",
        "\n",
        "#         return {\n",
        "#             \"sentence\": sentence,\n",
        "#             \"words\": words,\n",
        "#             \"labels\": labels,\n",
        "#         }"
      ],
      "metadata": {
        "id": "BW0Nu8_9V4Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_loader = PersianDatasetLoader()\n",
        "# sentences = dataset_loader.load_persian_datasets(max_sentences=100)"
      ],
      "metadata": {
        "id": "X2dRSmSNVrlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm import tqdm\n",
        "\n",
        "# labeler = OpenAIEzafeLabeler(OPENAI_API_KEY)\n",
        "# labeled_data = []\n",
        "\n",
        "# for sentence in tqdm(sentences):\n",
        "#     try:\n",
        "#       labeled_item = labeler.label_sentence(sentence)\n",
        "#       labeled_data.append(labeled_item)\n",
        "#     except:\n",
        "#       pass"
      ],
      "metadata": {
        "id": "tHzy7I77VXos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HomoRich Dataset"
      ],
      "metadata": {
        "id": "bvEvl2mz8hKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import itertools\n",
        "import random\n",
        "from typing import List, Dict\n",
        "\n",
        "FILE_URLS = [\n",
        "    \"https://huggingface.co/datasets/MahtaFetrat/HomoRich-G2P-Persian/resolve/main/data/part_01.parquet\",\n",
        "    \"https://huggingface.co/datasets/MahtaFetrat/HomoRich-G2P-Persian/resolve/main/data/part_02.parquet\",\n",
        "    \"https://huggingface.co/datasets/MahtaFetrat/HomoRich-G2P-Persian/resolve/main/data/part_03.parquet\"\n",
        "]\n",
        "\n",
        "def load_homorich(max_sentences: int = 20_000,\n",
        "                  file_urls=FILE_URLS,\n",
        "                  shuffle=True,\n",
        "                  seed=42) -> pd.DataFrame:\n",
        "    def read_shard(url: str) -> pd.DataFrame:\n",
        "        print(f\"Loading {url}\")\n",
        "        df = pd.read_parquet(url)\n",
        "\n",
        "        return df[[\"Grapheme\", \"Phoneme\"]]\n",
        "\n",
        "    frames = [read_shard(url) for url in file_urls]\n",
        "    df = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    if shuffle:\n",
        "        df = df.sample(frac=1, random_state=seed)\n",
        "\n",
        "    return df.head(max_sentences)"
      ],
      "metadata": {
        "id": "IUSDvlug85So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "homorich_df = load_homorich(max_sentences=50_000)\n",
        "homorich_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "qlfbodyVCGqw",
        "outputId": "a35a52e2-75ee-49bd-911d-a528ff83eef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading https://huggingface.co/datasets/MahtaFetrat/HomoRich-G2P-Persian/resolve/main/data/part_01.parquet\n",
            "Loading https://huggingface.co/datasets/MahtaFetrat/HomoRich-G2P-Persian/resolve/main/data/part_02.parquet\n",
            "Loading https://huggingface.co/datasets/MahtaFetrat/HomoRich-G2P-Persian/resolve/main/data/part_03.parquet\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 Grapheme  \\\n",
              "446698                         سی و یک، سی و دو، سی و سه.   \n",
              "310616              این قطعه از داستان را خیلی دوست دارم.   \n",
              "428128                         آیا می توانم چک شخصی بدهم؟   \n",
              "139706            یخچال پر از خوراکی‌های تازه و سالم بود.   \n",
              "438997                شستم خبر دار شد که کلکی در کار است.   \n",
              "...                                                   ...   \n",
              "328674           دوست داریم غذاهای اونجا رو امتحان کنیم .   \n",
              "103746  او در میان یادداشت‌های قدیمی‌اش ماند و به خاطر...   \n",
              "434030                     آیا هیچ بیماری های مزمن دارید؟   \n",
              "232107             او با اجرای خود، جشنواره را می‌ترکاند.   \n",
              "70315        کره ببر با بچه‌های دیگر در جنگل بازی می‌کرد.   \n",
              "\n",
              "                                                  Phoneme  \n",
              "446698                        si ?o yek si ?o do si ?o se  \n",
              "310616           ?in qat?e ?az dAstAn rA xeyli dust dAram  \n",
              "428128                 ?AyA mitavAnam Ceke Saxsi bedaham?  \n",
              "139706        yaxCAl por ?az xorAkihAye tAze va sAlem bud  \n",
              "438997        Sostam xabar dAr Sod ke kalaki dar kAr ?ast  \n",
              "...                                                   ...  \n",
              "328674        dust dArim qAzAhAye ?unjA ro ?emtehAn konim  \n",
              "103746  ?u dar miyAne yAddASthAye qadimi?aS mAnd va be...  \n",
              "434030                  ?AyA hiC bimArihAye mozmen dArid?  \n",
              "232107           ?u bA ?ejrAye xod jaSnvAre rA miterekAnd  \n",
              "70315   korreye babr bA baCCehAye digar dar jangl bAzi...  \n",
              "\n",
              "[50000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-037342a4-079c-48b0-b72d-11e384ad26e8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Grapheme</th>\n",
              "      <th>Phoneme</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>446698</th>\n",
              "      <td>سی و یک، سی و دو، سی و سه.</td>\n",
              "      <td>si ?o yek si ?o do si ?o se</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310616</th>\n",
              "      <td>این قطعه از داستان را خیلی دوست دارم.</td>\n",
              "      <td>?in qat?e ?az dAstAn rA xeyli dust dAram</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428128</th>\n",
              "      <td>آیا می توانم چک شخصی بدهم؟</td>\n",
              "      <td>?AyA mitavAnam Ceke Saxsi bedaham?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139706</th>\n",
              "      <td>یخچال پر از خوراکی‌های تازه و سالم بود.</td>\n",
              "      <td>yaxCAl por ?az xorAkihAye tAze va sAlem bud</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438997</th>\n",
              "      <td>شستم خبر دار شد که کلکی در کار است.</td>\n",
              "      <td>Sostam xabar dAr Sod ke kalaki dar kAr ?ast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>328674</th>\n",
              "      <td>دوست داریم غذاهای اونجا رو امتحان کنیم .</td>\n",
              "      <td>dust dArim qAzAhAye ?unjA ro ?emtehAn konim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103746</th>\n",
              "      <td>او در میان یادداشت‌های قدیمی‌اش ماند و به خاطر...</td>\n",
              "      <td>?u dar miyAne yAddASthAye qadimi?aS mAnd va be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434030</th>\n",
              "      <td>آیا هیچ بیماری های مزمن دارید؟</td>\n",
              "      <td>?AyA hiC bimArihAye mozmen dArid?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232107</th>\n",
              "      <td>او با اجرای خود، جشنواره را می‌ترکاند.</td>\n",
              "      <td>?u bA ?ejrAye xod jaSnvAre rA miterekAnd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70315</th>\n",
              "      <td>کره ببر با بچه‌های دیگر در جنگل بازی می‌کرد.</td>\n",
              "      <td>korreye babr bA baCCehAye digar dar jangl bAzi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-037342a4-079c-48b0-b72d-11e384ad26e8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-037342a4-079c-48b0-b72d-11e384ad26e8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-037342a4-079c-48b0-b72d-11e384ad26e8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-fa00f153-1600-46df-a745-7016cbad6b4d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fa00f153-1600-46df-a745-7016cbad6b4d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-fa00f153-1600-46df-a745-7016cbad6b4d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "homorich_df",
              "summary": "{\n  \"name\": \"homorich_df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"Grapheme\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 47394,\n        \"samples\": [\n          \"\\u0627\\u06cc\\u0646 \\u0644\\u0628\\u0627\\u0633\\u200c\\u0647\\u0627\\u06cc \\u062a\\u0631 \\u062a\\u0648\\u06cc \\u0633\\u0628\\u062f \\u0646\\u0645\\u0648\\u0646\\u0647\\u060c \\u0628\\u0647\\u062a\\u0631\\u0647 \\u067e\\u0647\\u0646 \\u0628\\u0634\\u0647.\",\n          \"\\u067e\\u06cc\\u0631\\u0645\\u0631\\u062f \\u0628\\u0627 \\u0686\\u0648\\u0628\\u062f\\u0633\\u062a\\u06cc\\u200c\\u0627\\u0634 \\u06a9\\u0644\\u0627\\u063a\\u200c\\u0647\\u0627 \\u0631\\u0627 \\u0627\\u0632 \\u0628\\u0627\\u063a\\u0686\\u0647 \\u0645\\u06cc\\u200c\\u067e\\u0631\\u0627\\u0646\\u062f.\",\n          \"\\u0627\\u0645\\u0644 \\u0628\\u0647 \\u0646\\u0648\\u06cc\\u0633\\u0646\\u062f\\u06af\\u06cc \\u0639\\u0644\\u0627\\u0642\\u0647 \\u062f\\u0627\\u0631\\u062f \\u0648 \\u062f\\u0631 \\u062d\\u0627\\u0644 \\u0646\\u0648\\u0634\\u062a\\u0646 \\u0627\\u0648\\u0644\\u06cc\\u0646 \\u0631\\u0645\\u0627\\u0646 \\u062e\\u0648\\u062f \\u0627\\u0633\\u062a.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Phoneme\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 48236,\n        \"samples\": [\n          \"deraxtAne kenAre jAdde ?az biAbi paZmordand\",\n          \"?eSkAl nadArad be SomA bepeyvandam?\",\n          \"dar hengAme jang ?u be keSvarhAye hamsAye farAr kard\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pos_tagger import SpacyPOSTagger\n",
        "\n",
        "spacy_posTagger = SpacyPOSTagger(model_path='spacy_pos_tagger_parsbertpostagger', using_gpu=True)\n",
        "preprocessor = PersianTextPreprocessor()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCcw8QlARjwF",
        "outputId": "09af1fd4-5edc-4f4f-bc16-55475adb218c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------ GPU Setup Process Started ---------------------\n",
            "------------ GPU is available and ready for use -------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import word_tokenize\n",
        "from tqdm import tqdm\n",
        "\n",
        "def infer_ezafe_labels(words: List[str], phonemes: List[str]) -> List[bool]:\n",
        "    labels = []\n",
        "    for i, ph in enumerate(phonemes):\n",
        "        if words[i].endswith(\"ه\") or i == len(phonemes) - 1:\n",
        "            labels.append(False)\n",
        "        else:\n",
        "            labels.append(ph.endswith(\"e\") or ph.endswith(\"ye\"))\n",
        "    return labels\n",
        "\n",
        "def infer_spacy_labels(sent: str) -> Tuple[List[str], List[bool]]:\n",
        "    words = word_tokenize(sent)\n",
        "    tagged = spacy_posTagger.tag(tokens=words, universal_tag=False)\n",
        "\n",
        "    spacy_words = []\n",
        "    ezafe_labels = []\n",
        "\n",
        "    for word, pos_tag in tagged:\n",
        "        spacy_words.append(word)\n",
        "        ezafe_labels.append(',EZ' in pos_tag)\n",
        "\n",
        "    return spacy_words, ezafe_labels\n",
        "\n"
      ],
      "metadata": {
        "id": "JGS2LwaDCQz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_data = homorich_to_labeled_list(homorich_df)\n",
        "print(f\"Kept {len(labeled_data):,} clean-aligned sentences\")"
      ],
      "metadata": {
        "id": "y16B61Vi8geu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d810ebdd-d55c-491f-f5d5-3fde3323a2f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing sentences: 100%|██████████| 50000/50000 [10:01<00:00, 83.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kept 49,656 clean-aligned sentences\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## verify and save dataset"
      ],
      "metadata": {
        "id": "1E0FIS2J_wfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item in labeled_data[5:15]:\n",
        "    print(f\"جمله: {item['sentence']}\")\n",
        "    print(f\"کلمات: {item['words']}\")\n",
        "    print(f\"برچسب‌ها: {item['labels']}\")\n",
        "    print(f\"تعداد کلمات: {len(item['words'])}\")\n",
        "    print(f\"تعداد برچسب‌ها: {len(item['labels'])}\")\n",
        "\n",
        "    print(\"تحلیل کلمه به کلمه:\")\n",
        "    for word, needs_ezafe in zip(item['words'], item['labels']):\n",
        "        ezafe_indicator = \"✓\" if needs_ezafe else \"✗\"\n",
        "        print(f\"  {word}: {ezafe_indicator}\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "bhspXSJAicHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7526b0ee-91c1-4e3f-fa14-cddba1ee3859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "جمله: در این پروژه طراحی، ایده‌های نوآورانه‌ای گنجاندند که تحسین‌برانگیز بود.\n",
            "کلمات: ['در', 'این', 'پروژه', 'طراحی', '،', 'ایده\\u200cهای', 'نوآورانه\\u200cای', 'گنجاندند', 'که', 'تحسین\\u200cبرانگیز', 'بود', '.']\n",
            "برچسب‌ها: [False, False, True, False, False, True, False, False, False, False, False, False]\n",
            "تعداد کلمات: 12\n",
            "تعداد برچسب‌ها: 12\n",
            "تحلیل کلمه به کلمه:\n",
            "  در: ✗\n",
            "  این: ✗\n",
            "  پروژه: ✓\n",
            "  طراحی: ✗\n",
            "  ،: ✗\n",
            "  ایده‌های: ✓\n",
            "  نوآورانه‌ای: ✗\n",
            "  گنجاندند: ✗\n",
            "  که: ✗\n",
            "  تحسین‌برانگیز: ✗\n",
            "  بود: ✗\n",
            "  .: ✗\n",
            "------------------------------------------------------------\n",
            "جمله: علی سونا می رفت و من بخور می دادم \n",
            "کلمات: ['علی', 'سونا', 'می', 'رفت', 'و', 'من', 'بخور', 'می', 'دادم']\n",
            "برچسب‌ها: [False, False, False, False, False, False, False, False, False]\n",
            "تعداد کلمات: 9\n",
            "تعداد برچسب‌ها: 9\n",
            "تحلیل کلمه به کلمه:\n",
            "  علی: ✗\n",
            "  سونا: ✗\n",
            "  می: ✗\n",
            "  رفت: ✗\n",
            "  و: ✗\n",
            "  من: ✗\n",
            "  بخور: ✗\n",
            "  می: ✗\n",
            "  دادم: ✗\n",
            "------------------------------------------------------------\n",
            "جمله: برخی گیاهان بدون نیاز به حشرات گرده‌افشانی می‌کنند.\n",
            "کلمات: ['برخی', 'گیاهان', 'بدون', 'نیاز', 'به', 'حشرات', 'گرده\\u200cافشانی', 'می\\u200cکنند', '.']\n",
            "برچسب‌ها: [False, False, True, False, False, False, False, False, False]\n",
            "تعداد کلمات: 9\n",
            "تعداد برچسب‌ها: 9\n",
            "تحلیل کلمه به کلمه:\n",
            "  برخی: ✗\n",
            "  گیاهان: ✗\n",
            "  بدون: ✓\n",
            "  نیاز: ✗\n",
            "  به: ✗\n",
            "  حشرات: ✗\n",
            "  گرده‌افشانی: ✗\n",
            "  می‌کنند: ✗\n",
            "  .: ✗\n",
            "------------------------------------------------------------\n",
            "جمله: دین می‌تواند در شکل‌گیری ارزش‌های فردی و اجتماعی نقش داشته باشد.\n",
            "کلمات: ['دین', 'می\\u200cتواند', 'در', 'شکل\\u200cگیری', 'ارزش\\u200cهای', 'فردی', 'و', 'اجتماعی', 'نقش', 'داشته_باشد', '.']\n",
            "برچسب‌ها: [False, False, False, True, True, False, False, False, False, False, False]\n",
            "تعداد کلمات: 11\n",
            "تعداد برچسب‌ها: 11\n",
            "تحلیل کلمه به کلمه:\n",
            "  دین: ✗\n",
            "  می‌تواند: ✗\n",
            "  در: ✗\n",
            "  شکل‌گیری: ✓\n",
            "  ارزش‌های: ✓\n",
            "  فردی: ✗\n",
            "  و: ✗\n",
            "  اجتماعی: ✗\n",
            "  نقش: ✗\n",
            "  داشته_باشد: ✗\n",
            "  .: ✗\n",
            "------------------------------------------------------------\n",
            "جمله: برای جلب اعتماد دیگران، نباید خلف وعده کنید.\n",
            "کلمات: ['برای', 'جلب', 'اعتماد', 'دیگران', '،', 'نباید', 'خلف', 'وعده', 'کنید', '.']\n",
            "برچسب‌ها: [True, True, True, False, False, False, True, False, False, False]\n",
            "تعداد کلمات: 10\n",
            "تعداد برچسب‌ها: 10\n",
            "تحلیل کلمه به کلمه:\n",
            "  برای: ✓\n",
            "  جلب: ✓\n",
            "  اعتماد: ✓\n",
            "  دیگران: ✗\n",
            "  ،: ✗\n",
            "  نباید: ✗\n",
            "  خلف: ✓\n",
            "  وعده: ✗\n",
            "  کنید: ✗\n",
            "  .: ✗\n",
            "------------------------------------------------------------\n",
            "جمله: در حمام سرد، بدنم به سرعت سر شد.\n",
            "کلمات: ['در', 'حمام', 'سرد', '،', 'بدنم', 'به', 'سرعت', 'سر', 'شد', '.']\n",
            "برچسب‌ها: [False, True, False, False, False, False, False, False, False, False]\n",
            "تعداد کلمات: 10\n",
            "تعداد برچسب‌ها: 10\n",
            "تحلیل کلمه به کلمه:\n",
            "  در: ✗\n",
            "  حمام: ✓\n",
            "  سرد: ✗\n",
            "  ،: ✗\n",
            "  بدنم: ✗\n",
            "  به: ✗\n",
            "  سرعت: ✗\n",
            "  سر: ✗\n",
            "  شد: ✗\n",
            "  .: ✗\n",
            "------------------------------------------------------------\n",
            "جمله: برات مهم نیست که اخراج شی\n",
            "کلمات: ['برات', 'مهم', 'نیست', 'که', 'اخراج', 'شی']\n",
            "برچسب‌ها: [False, False, False, False, False, False]\n",
            "تعداد کلمات: 6\n",
            "تعداد برچسب‌ها: 6\n",
            "تحلیل کلمه به کلمه:\n",
            "  برات: ✗\n",
            "  مهم: ✗\n",
            "  نیست: ✗\n",
            "  که: ✗\n",
            "  اخراج: ✗\n",
            "  شی: ✗\n",
            "------------------------------------------------------------\n",
            "جمله: او به دوستش کمک کرد تا چمدان سنگینش را هل دهد.\n",
            "کلمات: ['او', 'به', 'دوستش', 'کمک', 'کرد', 'تا', 'چمدان', 'سنگینش', 'را', 'هل', 'دهد', '.']\n",
            "برچسب‌ها: [False, False, False, False, False, False, True, False, False, False, False, False]\n",
            "تعداد کلمات: 12\n",
            "تعداد برچسب‌ها: 12\n",
            "تحلیل کلمه به کلمه:\n",
            "  او: ✗\n",
            "  به: ✗\n",
            "  دوستش: ✗\n",
            "  کمک: ✗\n",
            "  کرد: ✗\n",
            "  تا: ✗\n",
            "  چمدان: ✓\n",
            "  سنگینش: ✗\n",
            "  را: ✗\n",
            "  هل: ✗\n",
            "  دهد: ✗\n",
            "  .: ✗\n",
            "------------------------------------------------------------\n",
            "جمله: پیمان با عصبانیت حسین سر جا خود نشاند \n",
            "کلمات: ['پیمان', 'با', 'عصبانیت', 'حسین', 'سر', 'جا', 'خود', 'نشاند']\n",
            "برچسب‌ها: [False, False, False, False, True, True, False, False]\n",
            "تعداد کلمات: 8\n",
            "تعداد برچسب‌ها: 8\n",
            "تحلیل کلمه به کلمه:\n",
            "  پیمان: ✗\n",
            "  با: ✗\n",
            "  عصبانیت: ✗\n",
            "  حسین: ✗\n",
            "  سر: ✓\n",
            "  جا: ✓\n",
            "  خود: ✗\n",
            "  نشاند: ✗\n",
            "------------------------------------------------------------\n",
            "جمله: داشتین با یه تخته سنگ حرف می زدین\n",
            "کلمات: ['داشتین', 'با', 'یه', 'تخته', 'سنگ', 'حرف', 'می', 'زدین']\n",
            "برچسب‌ها: [False, False, False, False, False, False, False, False]\n",
            "تعداد کلمات: 8\n",
            "تعداد برچسب‌ها: 8\n",
            "تحلیل کلمه به کلمه:\n",
            "  داشتین: ✗\n",
            "  با: ✗\n",
            "  یه: ✗\n",
            "  تخته: ✗\n",
            "  سنگ: ✗\n",
            "  حرف: ✗\n",
            "  می: ✗\n",
            "  زدین: ✗\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir drive\n",
        "!mkdir drive/MyDrive"
      ],
      "metadata": {
        "id": "iUABGIa_Nd1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/labeled_data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(labeled_data, f, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "icYfUoxB_pj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training"
      ],
      "metadata": {
        "id": "g4Dy42y79BU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EzafeDataset(Dataset):\n",
        "    def __init__(self, labeled_data, tokenizer, max_length=128):\n",
        "        self.data = labeled_data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        words = item['words']\n",
        "        labels = item['labels']\n",
        "\n",
        "        label_ids = [1 if label else 0 for label in labels]\n",
        "\n",
        "        tokenized = self.tokenizer(\n",
        "            words,\n",
        "            is_split_into_words=True,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        word_ids = tokenized.word_ids()\n",
        "        aligned_labels = []\n",
        "\n",
        "        previous_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                aligned_labels.append(-100)  # Ignore padding tokens\n",
        "            elif word_idx != previous_word_idx:\n",
        "                if word_idx < len(label_ids):\n",
        "                    aligned_labels.append(label_ids[word_idx])\n",
        "                else:\n",
        "                    aligned_labels.append(-100)\n",
        "            else:\n",
        "                aligned_labels.append(-100)  # Ignore subword tokens\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        return {\n",
        "            'input_ids': tokenized['input_ids'].squeeze(),\n",
        "            'attention_mask': tokenized['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(aligned_labels, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "hw_6xpHglOAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "model_name = \"HooshvareLab/albert-fa-zwnj-base-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "config.hidden_dropout_prob = 0.2\n",
        "config.attention_probs_dropout_prob = 0.2\n",
        "\n",
        "config.num_labels = 2\n",
        "config.id2label = {0: \"NO_EZAFE\", 1: \"NEEDS_EZAFE\"}\n",
        "config.label2id = {\"NO_EZAFE\": 0, \"NEEDS_EZAFE\": 1}\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    config=config\n",
        ")"
      ],
      "metadata": {
        "id": "aatxSOialjL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b6b946a-7787-4731-ef5d-1a839fb06ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at HooshvareLab/albert-fa-zwnj-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, val_data = train_test_split(labeled_data, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = EzafeDataset(train_data, tokenizer)\n",
        "val_dataset = EzafeDataset(val_data, tokenizer)\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/ezafe_model',\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=2e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='/content/logs',\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_steps=400,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_f1\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True,\n",
        "    dataloader_pin_memory=False,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=[],\n",
        ")"
      ],
      "metadata": {
        "id": "PYc_tz6ukob0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [l for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    flat_predictions = [item for sublist in true_predictions for item in sublist]\n",
        "    flat_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(flat_labels, flat_predictions, average='weighted')\n",
        "    accuracy = accuracy_score(flat_labels, flat_predictions)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "a_GvM6rYKB46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "model.save_pretrained('/content/drive/MyDrive/ezafe_model_final')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/ezafe_model_final')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 914
        },
        "id": "vhA3FfF7lZz_",
        "outputId": "8863c5f8-64b6-45e4-d787-a9ba52ad5197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4968' max='4968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4968/4968 35:53, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.440900</td>\n",
              "      <td>0.305491</td>\n",
              "      <td>0.846641</td>\n",
              "      <td>0.799880</td>\n",
              "      <td>0.838880</td>\n",
              "      <td>0.846641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.238800</td>\n",
              "      <td>0.121600</td>\n",
              "      <td>0.957271</td>\n",
              "      <td>0.957135</td>\n",
              "      <td>0.957024</td>\n",
              "      <td>0.957271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.172300</td>\n",
              "      <td>0.084419</td>\n",
              "      <td>0.968852</td>\n",
              "      <td>0.968933</td>\n",
              "      <td>0.969031</td>\n",
              "      <td>0.968852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.141400</td>\n",
              "      <td>0.065956</td>\n",
              "      <td>0.976498</td>\n",
              "      <td>0.976456</td>\n",
              "      <td>0.976421</td>\n",
              "      <td>0.976498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.117100</td>\n",
              "      <td>0.058259</td>\n",
              "      <td>0.979018</td>\n",
              "      <td>0.978981</td>\n",
              "      <td>0.978951</td>\n",
              "      <td>0.979018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.107500</td>\n",
              "      <td>0.054133</td>\n",
              "      <td>0.980922</td>\n",
              "      <td>0.980971</td>\n",
              "      <td>0.981037</td>\n",
              "      <td>0.980922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.096400</td>\n",
              "      <td>0.055166</td>\n",
              "      <td>0.981379</td>\n",
              "      <td>0.981437</td>\n",
              "      <td>0.981520</td>\n",
              "      <td>0.981379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.082900</td>\n",
              "      <td>0.050628</td>\n",
              "      <td>0.982528</td>\n",
              "      <td>0.982585</td>\n",
              "      <td>0.982670</td>\n",
              "      <td>0.982528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.083900</td>\n",
              "      <td>0.046739</td>\n",
              "      <td>0.984112</td>\n",
              "      <td>0.984100</td>\n",
              "      <td>0.984089</td>\n",
              "      <td>0.984112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.078200</td>\n",
              "      <td>0.045121</td>\n",
              "      <td>0.984474</td>\n",
              "      <td>0.984470</td>\n",
              "      <td>0.984466</td>\n",
              "      <td>0.984474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.074300</td>\n",
              "      <td>0.042548</td>\n",
              "      <td>0.985271</td>\n",
              "      <td>0.985292</td>\n",
              "      <td>0.985317</td>\n",
              "      <td>0.985271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.075800</td>\n",
              "      <td>0.042792</td>\n",
              "      <td>0.985250</td>\n",
              "      <td>0.985304</td>\n",
              "      <td>0.985393</td>\n",
              "      <td>0.985250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>0.041966</td>\n",
              "      <td>0.986239</td>\n",
              "      <td>0.986258</td>\n",
              "      <td>0.986282</td>\n",
              "      <td>0.986239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.067100</td>\n",
              "      <td>0.040423</td>\n",
              "      <td>0.986026</td>\n",
              "      <td>0.986060</td>\n",
              "      <td>0.986107</td>\n",
              "      <td>0.986026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.062000</td>\n",
              "      <td>0.040915</td>\n",
              "      <td>0.986303</td>\n",
              "      <td>0.986285</td>\n",
              "      <td>0.986271</td>\n",
              "      <td>0.986303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.061700</td>\n",
              "      <td>0.039281</td>\n",
              "      <td>0.986324</td>\n",
              "      <td>0.986328</td>\n",
              "      <td>0.986332</td>\n",
              "      <td>0.986324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.060700</td>\n",
              "      <td>0.038537</td>\n",
              "      <td>0.986898</td>\n",
              "      <td>0.986902</td>\n",
              "      <td>0.986906</td>\n",
              "      <td>0.986898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.057300</td>\n",
              "      <td>0.038900</td>\n",
              "      <td>0.987207</td>\n",
              "      <td>0.987208</td>\n",
              "      <td>0.987208</td>\n",
              "      <td>0.987207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>0.057000</td>\n",
              "      <td>0.038025</td>\n",
              "      <td>0.987058</td>\n",
              "      <td>0.987063</td>\n",
              "      <td>0.987068</td>\n",
              "      <td>0.987058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.057300</td>\n",
              "      <td>0.038213</td>\n",
              "      <td>0.987037</td>\n",
              "      <td>0.987027</td>\n",
              "      <td>0.987019</td>\n",
              "      <td>0.987037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>0.055300</td>\n",
              "      <td>0.038140</td>\n",
              "      <td>0.987302</td>\n",
              "      <td>0.987304</td>\n",
              "      <td>0.987305</td>\n",
              "      <td>0.987302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>0.054500</td>\n",
              "      <td>0.038099</td>\n",
              "      <td>0.987313</td>\n",
              "      <td>0.987317</td>\n",
              "      <td>0.987322</td>\n",
              "      <td>0.987313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>0.055200</td>\n",
              "      <td>0.037937</td>\n",
              "      <td>0.987313</td>\n",
              "      <td>0.987312</td>\n",
              "      <td>0.987310</td>\n",
              "      <td>0.987313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>0.053100</td>\n",
              "      <td>0.038024</td>\n",
              "      <td>0.987249</td>\n",
              "      <td>0.987254</td>\n",
              "      <td>0.987260</td>\n",
              "      <td>0.987249</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/ezafe_model_final/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/ezafe_model_final/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/ezafe_model_final/spiece.model',\n",
              " '/content/drive/MyDrive/ezafe_model_final/added_tokens.json',\n",
              " '/content/drive/MyDrive/ezafe_model_final/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/ezafe_model_final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMp9tgD1nZRG",
        "outputId": "7f1ecfcd-6aa0-492d-d6e0-e3183e635233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json\t   special_tokens_map.json  tokenizer_config.json\n",
            "model.safetensors  spiece.model\t\t    tokenizer.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmCn1gx4mfFC",
        "outputId": "a8718a3d-36c4-45e3-ca93-17f5dce6ec0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/drive/MyDrive/ezafe_model_final /content/drive2/MyDrive/ezafe_model_final"
      ],
      "metadata": {
        "id": "2ecCBTb5nVRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_ezafe(text, model, tokenizer):\n",
        "    preprocessor = PersianTextPreprocessor()\n",
        "    words = preprocessor.tokenize_words(text)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        words,\n",
        "        is_split_into_words=True,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    device = model.device\n",
        "    inputs_on_device = {k: v.to(device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs_on_device)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_labels = torch.argmax(predictions, dim=-1)\n",
        "\n",
        "    word_ids = inputs.word_ids()\n",
        "    word_predictions = []\n",
        "\n",
        "    previous_word_idx = None\n",
        "    for i, word_idx in enumerate(word_ids):\n",
        "        if word_idx is not None and word_idx != previous_word_idx:\n",
        "            if word_idx < len(words):\n",
        "                word_predictions.append({\n",
        "                    'word': words[word_idx],\n",
        "                    'needs_ezafe': bool(predicted_labels[0][i].item()),\n",
        "                    'confidence': float(predictions[0][i][predicted_labels[0][i]].item())\n",
        "                })\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "    return word_predictions"
      ],
      "metadata": {
        "id": "x2AWU4ZZo2aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoO9rg7rnot3",
        "outputId": "1dee3dcb-fd98-4545-b350-2724a465b778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'word': 'سنگینی', 'needs_ezafe': True, 'confidence': 0.9616780281066895},\n",
              " {'word': 'موفقیت', 'needs_ezafe': False, 'confidence': 0.9805247783660889},\n",
              " {'word': 'جای', 'needs_ezafe': True, 'confidence': 0.9994866847991943},\n",
              " {'word': 'خود', 'needs_ezafe': False, 'confidence': 0.9997774958610535},\n",
              " {'word': 'را', 'needs_ezafe': False, 'confidence': 0.9999128580093384},\n",
              " {'word': 'به', 'needs_ezafe': False, 'confidence': 0.9999004602432251},\n",
              " {'word': 'سبکی', 'needs_ezafe': False, 'confidence': 0.996906578540802},\n",
              " {'word': 'تازهکار', 'needs_ezafe': False, 'confidence': 0.9941118359565735},\n",
              " {'word': 'بودن', 'needs_ezafe': False, 'confidence': 0.9993650317192078},\n",
              " {'word': 'داده', 'needs_ezafe': False, 'confidence': 0.9999490976333618},\n",
              " {'word': 'بود', 'needs_ezafe': False, 'confidence': 0.9997958540916443},\n",
              " {'word': 'حالا', 'needs_ezafe': False, 'confidence': 0.9999053478240967},\n",
              " {'word': 'نسبتبه', 'needs_ezafe': False, 'confidence': 0.9978384375572205},\n",
              " {'word': 'همه', 'needs_ezafe': False, 'confidence': 0.9634719491004944},\n",
              " {'word': 'چیز', 'needs_ezafe': False, 'confidence': 0.9476723670959473},\n",
              " {'word': 'کمتر', 'needs_ezafe': False, 'confidence': 0.998217761516571},\n",
              " {'word': 'مطمین', 'needs_ezafe': False, 'confidence': 0.9996095299720764},\n",
              " {'word': 'بودم', 'needs_ezafe': False, 'confidence': 0.9999027252197266},\n",
              " {'word': 'و', 'needs_ezafe': False, 'confidence': 0.9998433589935303},\n",
              " {'word': 'همین', 'needs_ezafe': False, 'confidence': 0.999904990196228},\n",
              " {'word': 'باعث', 'needs_ezafe': False, 'confidence': 0.9993525147438049},\n",
              " {'word': 'شد', 'needs_ezafe': False, 'confidence': 0.9997005462646484},\n",
              " {'word': 'وارد', 'needs_ezafe': True, 'confidence': 0.9584211707115173},\n",
              " {'word': 'یکی', 'needs_ezafe': False, 'confidence': 0.9997336268424988},\n",
              " {'word': 'از', 'needs_ezafe': False, 'confidence': 0.9999068975448608},\n",
              " {'word': 'خلاقانهترین',\n",
              "  'needs_ezafe': False,\n",
              "  'confidence': 0.9979292154312134},\n",
              " {'word': 'دورههای', 'needs_ezafe': True, 'confidence': 0.9978594183921814},\n",
              " {'word': 'زندگیام', 'needs_ezafe': False, 'confidence': 0.9961680769920349},\n",
              " {'word': 'شوم', 'needs_ezafe': False, 'confidence': 0.9996886253356934}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"سنگینی موفقیت جای خود را به سبُکی تازه‌کار بودن داده بود. حالا نسبت‌به همه چیز کمتر مطمئن بودم و همین باعث شد وارد یکی از خلاقانه‌ترین دوره‌های زندگی‌ام شوم.\"\n",
        "results = predict_ezafe(test_sentence, model, tokenizer)\n",
        "print(f\"\\nTest sentence: {test_sentence}\")\n",
        "for result in results:\n",
        "    ezafe_indicator = \"✓\" if result['needs_ezafe'] else \"✗\"\n",
        "    print(f\"{result['word']}: {ezafe_indicator} (confidence: {result['confidence']:.3f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkaJFY2AlH7g",
        "outputId": "2cbd3521-6343-424d-ed84-1421a178d8f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test sentence: سنگینی موفقیت جای خود را به سبُکی تازه‌کار بودن داده بود. حالا نسبت‌به همه چیز کمتر مطمئن بودم و همین باعث شد وارد یکی از خلاقانه‌ترین دوره‌های زندگی‌ام شوم.\n",
            "سنگینی: ✓ (confidence: 0.962)\n",
            "موفقیت: ✗ (confidence: 0.981)\n",
            "جای: ✓ (confidence: 0.999)\n",
            "خود: ✗ (confidence: 1.000)\n",
            "را: ✗ (confidence: 1.000)\n",
            "به: ✗ (confidence: 1.000)\n",
            "سبکی: ✗ (confidence: 0.997)\n",
            "تازهکار: ✗ (confidence: 0.994)\n",
            "بودن: ✗ (confidence: 0.999)\n",
            "داده: ✗ (confidence: 1.000)\n",
            "بود: ✗ (confidence: 1.000)\n",
            "حالا: ✗ (confidence: 1.000)\n",
            "نسبتبه: ✗ (confidence: 0.998)\n",
            "همه: ✗ (confidence: 0.963)\n",
            "چیز: ✗ (confidence: 0.948)\n",
            "کمتر: ✗ (confidence: 0.998)\n",
            "مطمین: ✗ (confidence: 1.000)\n",
            "بودم: ✗ (confidence: 1.000)\n",
            "و: ✗ (confidence: 1.000)\n",
            "همین: ✗ (confidence: 1.000)\n",
            "باعث: ✗ (confidence: 0.999)\n",
            "شد: ✗ (confidence: 1.000)\n",
            "وارد: ✓ (confidence: 0.958)\n",
            "یکی: ✗ (confidence: 1.000)\n",
            "از: ✗ (confidence: 1.000)\n",
            "خلاقانهترین: ✗ (confidence: 0.998)\n",
            "دورههای: ✓ (confidence: 0.998)\n",
            "زندگیام: ✗ (confidence: 0.996)\n",
            "شوم: ✗ (confidence: 1.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## export quantized onnx"
      ],
      "metadata": {
        "id": "0ZXR4gz3JKeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!optimum-cli export onnx \\\n",
        "  --model /content/drive/MyDrive/ezafe_model_final \\\n",
        "  --task token-classification \\\n",
        "  --opset 20 \\\n",
        "  /content/drive/MyDrive/ezafe_model_onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvD0oyoSlK8Q",
        "outputId": "fd903e19-eb1a-4573-f9ac-91445e0b5da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-05 15:38:03.759758: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1749137883.781509   23781 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1749137883.788302   23781 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!optimum-cli onnxruntime quantize \\\n",
        "  --avx512 \\\n",
        "  --onnx_model /content/drive/MyDrive/ezafe_model_onnx \\\n",
        "  -o /content/drive/MyDrive/ezafe_model_quantized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weSQptkr0wLB",
        "outputId": "9a2ca506-58dc-45b8-fa4d-52fda2d39259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-05 15:38:23.055933: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1749137903.075603   23875 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1749137903.081659   23875 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/\n",
        "!zip -r ezafe_model_final.zip ezafe_model_final/\n",
        "!zip -r ezafe_model_onnx.zip ezafe_model_onnx/\n",
        "!zip -r ezafe_model_quantized.zip ezafe_model_quantized/\n",
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOtVXYxHLBN8",
        "outputId": "4df75efb-df8d-4d6c-a63f-8394195edc90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "  adding: ezafe_model_final/ (stored 0%)\n",
            "  adding: ezafe_model_final/config.json (deflated 52%)\n",
            "  adding: ezafe_model_final/model.safetensors (deflated 7%)\n",
            "  adding: ezafe_model_final/tokenizer_config.json (deflated 95%)\n",
            "  adding: ezafe_model_final/special_tokens_map.json (deflated 85%)\n",
            "  adding: ezafe_model_final/spiece.model (deflated 54%)\n",
            "  adding: ezafe_model_final/tokenizer.json (deflated 75%)\n",
            "  adding: ezafe_model_onnx/ (stored 0%)\n",
            "  adding: ezafe_model_onnx/config.json (deflated 52%)\n",
            "  adding: ezafe_model_onnx/tokenizer_config.json (deflated 95%)\n",
            "  adding: ezafe_model_onnx/special_tokens_map.json (deflated 85%)\n",
            "  adding: ezafe_model_onnx/spiece.model (deflated 54%)\n",
            "  adding: ezafe_model_onnx/tokenizer.json (deflated 75%)\n",
            "  adding: ezafe_model_onnx/model.onnx (deflated 8%)\n",
            "  adding: ezafe_model_quantized/ (stored 0%)\n",
            "  adding: ezafe_model_quantized/model_quantized.onnx (deflated 12%)\n",
            "  adding: ezafe_model_quantized/ort_config.json (deflated 56%)\n",
            "  adding: ezafe_model_quantized/config.json (deflated 52%)\n",
            "  adding: ezafe_model_quantized/tokenizer_config.json (deflated 95%)\n",
            "  adding: ezafe_model_quantized/special_tokens_map.json (deflated 85%)\n",
            "  adding: ezafe_model_quantized/spiece.model (deflated 54%)\n",
            "  adding: ezafe_model_quantized/tokenizer.json (deflated 75%)\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test quantized version"
      ],
      "metadata": {
        "id": "cSB9e2ehJC8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model_path = \"/content/drive/MyDrive/ezafe_model_final\"\n",
        "quantized_model_path = \"/content/drive/MyDrive/ezafe_model_quantized\""
      ],
      "metadata": {
        "id": "2NnadWMqAuZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.onnxruntime import ORTModelForTokenClassification\n",
        "\n",
        "quantized_model = ORTModelForTokenClassification.from_pretrained(quantized_model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(final_model_path)"
      ],
      "metadata": {
        "id": "pFIzLcgh8UVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_ezafe_simple(text, model, tokenizer):\n",
        "    words = text.split()\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        words,\n",
        "        is_split_into_words=True,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_labels = torch.argmax(predictions, dim=-1)\n",
        "\n",
        "    word_ids = inputs.word_ids()\n",
        "    results = []\n",
        "\n",
        "    previous_word_idx = None\n",
        "    for i, word_idx in enumerate(word_ids):\n",
        "        if word_idx is not None and word_idx != previous_word_idx:\n",
        "            if word_idx < len(words):\n",
        "                results.append({\n",
        "                    'word': words[word_idx],\n",
        "                    'needs_ezafe': bool(predicted_labels[0][i].item()),\n",
        "                    'confidence': float(predictions[0][i][predicted_labels[0][i]].item())\n",
        "                })\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "SyhceF4E8XXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"سنگینی موفقیت جای خود را به سبُکی تازه‌کار بودن داده بود.\",\n",
        "    \"حالا نسبت‌به همه چیز کمتر مطمئن بودم و همین باعث شد وارد یکی از خلاقانه‌ترین دوره‌های زندگی‌ام شوم.\",\n",
        "    \"این یک جمله آزمایشی برای تست سرعت و دقت مدل است.\",\n",
        "    \"کتاب‌های علمی دانشگاه بسیار مفید هستند.\",\n",
        "    \"دانشجویان ایرانی در رشته‌های مختلف تحصیل می‌کنند.\"\n",
        "]\n",
        "\n",
        "for test_sentence in test_sentences:\n",
        "    results = predict_ezafe_simple(test_sentence, quantized_model, tokenizer)\n",
        "\n",
        "    print(f\"Test sentence: {test_sentence}\\n\")\n",
        "    print(\"Ezafe predictions:\")\n",
        "    for result in results:\n",
        "        ezafe_indicator = \"✓\" if result['needs_ezafe'] else \"✗\"\n",
        "        print(f\"{result['word']}: {ezafe_indicator} (confidence: {result['confidence']:.3f})\")\n",
        "    print()\n",
        "    print()"
      ],
      "metadata": {
        "id": "3mQFtes_AOfN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0017b7cf-b66f-4291-feea-97c0be214a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test sentence: سنگینی موفقیت جای خود را به سبُکی تازه‌کار بودن داده بود.\n",
            "\n",
            "Ezafe predictions:\n",
            "سنگینی: ✓ (confidence: 0.989)\n",
            "موفقیت: ✗ (confidence: 0.994)\n",
            "جای: ✓ (confidence: 1.000)\n",
            "خود: ✗ (confidence: 1.000)\n",
            "را: ✗ (confidence: 1.000)\n",
            "به: ✗ (confidence: 1.000)\n",
            "سبُکی: ✗ (confidence: 0.994)\n",
            "تازه‌کار: ✗ (confidence: 0.996)\n",
            "بودن: ✗ (confidence: 0.999)\n",
            "داده: ✗ (confidence: 1.000)\n",
            "بود.: ✗ (confidence: 1.000)\n",
            "\n",
            "\n",
            "Test sentence: حالا نسبت‌به همه چیز کمتر مطمئن بودم و همین باعث شد وارد یکی از خلاقانه‌ترین دوره‌های زندگی‌ام شوم.\n",
            "\n",
            "Ezafe predictions:\n",
            "حالا: ✗ (confidence: 1.000)\n",
            "نسبت‌به: ✗ (confidence: 0.515)\n",
            "همه: ✗ (confidence: 0.980)\n",
            "چیز: ✗ (confidence: 0.996)\n",
            "کمتر: ✗ (confidence: 0.999)\n",
            "مطمئن: ✗ (confidence: 0.998)\n",
            "بودم: ✗ (confidence: 1.000)\n",
            "و: ✗ (confidence: 1.000)\n",
            "همین: ✗ (confidence: 1.000)\n",
            "باعث: ✗ (confidence: 1.000)\n",
            "شد: ✗ (confidence: 1.000)\n",
            "وارد: ✓ (confidence: 0.998)\n",
            "یکی: ✗ (confidence: 1.000)\n",
            "از: ✗ (confidence: 1.000)\n",
            "خلاقانه‌ترین: ✗ (confidence: 0.999)\n",
            "دوره‌های: ✓ (confidence: 0.999)\n",
            "زندگی‌ام: ✗ (confidence: 0.999)\n",
            "شوم.: ✗ (confidence: 1.000)\n",
            "\n",
            "\n",
            "Test sentence: این یک جمله آزمایشی برای تست سرعت و دقت مدل است.\n",
            "\n",
            "Ezafe predictions:\n",
            "این: ✗ (confidence: 1.000)\n",
            "یک: ✗ (confidence: 1.000)\n",
            "جمله: ✗ (confidence: 0.671)\n",
            "آزمایشی: ✗ (confidence: 0.996)\n",
            "برای: ✓ (confidence: 0.999)\n",
            "تست: ✓ (confidence: 0.999)\n",
            "سرعت: ✗ (confidence: 0.997)\n",
            "و: ✗ (confidence: 1.000)\n",
            "دقت: ✓ (confidence: 0.915)\n",
            "مدل: ✗ (confidence: 0.999)\n",
            "است.: ✗ (confidence: 1.000)\n",
            "\n",
            "\n",
            "Test sentence: کتاب‌های علمی دانشگاه بسیار مفید هستند.\n",
            "\n",
            "Ezafe predictions:\n",
            "کتاب‌های: ✓ (confidence: 1.000)\n",
            "علمی: ✓ (confidence: 0.999)\n",
            "دانشگاه: ✗ (confidence: 1.000)\n",
            "بسیار: ✗ (confidence: 1.000)\n",
            "مفید: ✗ (confidence: 1.000)\n",
            "هستند.: ✗ (confidence: 1.000)\n",
            "\n",
            "\n",
            "Test sentence: دانشجویان ایرانی در رشته‌های مختلف تحصیل می‌کنند.\n",
            "\n",
            "Ezafe predictions:\n",
            "دانشجویان: ✓ (confidence: 0.999)\n",
            "ایرانی: ✗ (confidence: 1.000)\n",
            "در: ✗ (confidence: 1.000)\n",
            "رشته‌های: ✓ (confidence: 1.000)\n",
            "مختلف: ✗ (confidence: 0.999)\n",
            "تحصیل: ✗ (confidence: 1.000)\n",
            "می‌کنند.: ✗ (confidence: 1.000)\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}